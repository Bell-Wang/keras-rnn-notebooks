{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#6629b2'>Generating text with recurrent neural networks using Keras</font>\n",
    "by Melissa Roemmele, 7/17/17, roemmele @ usc.edu\n",
    "\n",
    "## <font color='#6629b2'>Overview</font>\n",
    "\n",
    "I am going to show how to build a recurrent neural network (RNN) language model that learns the relation between words in text, using the Keras library for machine learning. I will then show how this model can be used for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Recurrent Neural Networks (RNNs)</font>\n",
    "\n",
    "RNNs are a general framework for modeling sequence data and are particularly useful for natural langugage processing tasks. Here an RNN will be used as a language model, which can predict which word is likely to occur next in a text given the words before it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Keras</font>\n",
    "\n",
    "[Keras](https://keras.io/) is a Python deep learning framework that lets you quickly put together neural network models with a minimal amount of code. It can be run on top of [Theano](http://deeplearning.net/software/theano/) or [Tensor Flow](https://www.tensorflow.org/) without you needing to know either of these underlying frameworks. It provides implementations of several of the layer architectures, objective functions, and optimization algorithms you need for building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Dataset</font>\n",
    "\n",
    "My research is on story generation, so I've selected a dataset of stories as the text to be modeled by the RNN. They come from the [ROCStories](http://cs.rochester.edu/nlp/rocstories/) dataset, which consists of thousands of five-sentence stories about everyday life events. Here the model will observe all five sentences in each story. Then we'll use the trained model to generate the final sentence in a set of stories not observed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''load the dataset'''\n",
    "import csv\n",
    "\n",
    "with open('ROCStories_winter2017.csv', 'r') as f:\n",
    "    stories = [story[2:] for story in csv.reader(f)] #first two columns are ID and story title, ignore these\n",
    "    stories = stories[1:] #first row is column names, ignore this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David noticed he had put on a lot of weight recently. He examined his habits to try and figure out the reason. He realized he'd been eating too much fast food lately. He stopped going to burger places and started a vegetarian diet. After a few weeks, he started to feel much better.\n",
      "\n",
      "Tom had a very short temper. One day a guest made him very angry. He punched a hole in the wall of his house. Tom's guest became afraid and left quickly. Tom sat on his couch filled with regret about his actions.\n"
     ]
    }
   ],
   "source": [
    "'''designate a set of training stories'''\n",
    "\n",
    "train_stories = stories[:100]\n",
    "train_stories = [\" \".join(story) for story in train_stories] #concatenate sentences in each story into one long string\n",
    "print \"\\n\\n\".join(train_stories[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Preparing the data</font>\n",
    "\n",
    "The model we'll create is a word-based language model, which means each input unit is a single word (some language models learn subword units like characters). \n",
    "\n",
    "So first we need to tokenize each of the stories into (lowercased) individual words. I'll use Keras' built-in tokenizer here for convenience, but typically I like to use [spacy](https://spacy.io/), a fast and user-friendly library that performs various language processing tasks. \n",
    "\n",
    "A note: Keras' tokenizer does not do the same linguistic processing to separate punctuation from words, for instance, which should be their own tokens. You can see this below from words that end in punctuation like \".\" or \",\".\n",
    "\n",
    "We need to assemble a lexicon (aka vocabulary) of words that the model needs to know. Thus, each tokenized word in the stories is added to the lexicon. We use the fit_on_texts() function to map each word in the stories to a numerical index. When working with large datasets it's common to filter all words occurring less than a certain number of times, and replace them with some generic \"UNKNOWN\" token. Here, because this dataset is small, every word encountered in the stories is added to the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raining: 533\n",
      "better,: 534\n",
      "better.: 535\n",
      "four: 536\n",
      "portland.: 537\n",
      "protest: 538\n",
      "sleep: 191\n",
      "ocean!: 539\n",
      "party.: 129\n",
      "up.: 270\n",
      "up,: 540\n",
      "electricity: 271\n",
      "up!: 541\n",
      "presents: 272\n",
      "under: 542\n",
      "worth: 543\n",
      "advice.: 544\n",
      "every: 100\n",
      "today.: 273\n",
      "skills: 545\n"
     ]
    }
   ],
   "source": [
    "'''make the lexicon'''\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(lower=True, filters='')\n",
    "tokenizer.fit_on_texts(train_stories) #split stories into words, assign number to each unique word\n",
    "print \"\\n\".join([word + \": \" + str(word_idx) for word, word_idx \n",
    "                 in tokenizer.word_index.items()[:20]]) #print a sample of the dictionary\n",
    "\n",
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as f: #save the tokenizer\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David noticed he had put on a lot of weight recently. He examined his habits to try and figure out the reason. He realized he'd been eating too much fast food lately. He stopped going to burger places and started a vegetarian diet. After a few weeks, he started to feel much better. \n",
      "\n",
      "[291, 189, 4, 13, 164, 16, 3, 91, 11, 1098, 814, 4, 1440, 8, 1095, 2, 125, 6, 824, 36, 1, 1197, 4, 175, 371, 109, 467, 106, 60, 214, 162, 1360, 4, 238, 49, 2, 1052, 1499, 6, 46, 3, 1063, 522, 37, 3, 126, 1165, 4, 46, 2, 235, 60, 535]\n"
     ]
    }
   ],
   "source": [
    "'''convert each story from text to numbers'''\n",
    "\n",
    "train_idxs = tokenizer.texts_to_sequences(train_stories) #transform each word to its numerical index in lexicon\n",
    "print train_stories[0], \"\\n\"\n",
    "print train_idxs[0] #show example of encoded story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Creating a matrix</font>\n",
    "\n",
    "Finally, we need to put all the training stories into a single matrix, where each row is a story and each column is a word index in that story. This enables the model to process the stories in batches as opposed to one at a time, which significantly speeds up training. However, each story has a different number of words. So we create a padded matrix equal to the length on the longest story in the training set. For all stories with fewer words, we prepend the row with zeros representing an empty word position. Then we can actually tell Keras to ignore these zeros during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix length: 64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "        291,  189,    4,   13,  164,   16,    3,   91,   11, 1098,  814,\n",
       "          4, 1440,    8, 1095,    2,  125,    6,  824,   36,    1, 1197,\n",
       "          4,  175,  371,  109,  467,  106,   60,  214,  162, 1360,    4,\n",
       "        238,   49,    2, 1052, 1499,    6,   46,    3, 1063,  522,   37,\n",
       "          3,  126, 1165,    4,   46,    2,  235,   60,  535], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''create a padded matrix of stories'''\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = max([len(story) for story in train_idxs]) # get length of longest story\n",
    "print \"matrix length:\", maxlen\n",
    "\n",
    "train_idxs = pad_sequences(train_idxs, maxlen=maxlen) #keras provides convenient padding function\n",
    "train_idxs[0] #same example story as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Defining the input and output</font>\n",
    "\n",
    "In an RNN language model, the data is set up so that each word in the text is mapped to the word that follows it. In a given story, for each input word x[idx], the output label y[idx] is just x[idx+1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [   0    0    0    0    0    0    0    0    0    0    0  291  189    4   13\n",
      "  164   16    3   91   11 1098  814    4 1440    8 1095    2  125    6  824\n",
      "   36    1 1197    4  175  371  109  467  106   60  214  162 1360    4  238\n",
      "   49    2 1052 1499    6   46    3 1063  522   37    3  126 1165    4   46\n",
      "    2  235   60]\n",
      "y: [   0    0    0    0    0    0    0    0    0    0  291  189    4   13  164\n",
      "   16    3   91   11 1098  814    4 1440    8 1095    2  125    6  824   36\n",
      "    1 1197    4  175  371  109  467  106   60  214  162 1360    4  238   49\n",
      "    2 1052 1499    6   46    3 1063  522   37    3  126 1165    4   46    2\n",
      "  235   60  535]\n"
     ]
    }
   ],
   "source": [
    "'''set up the model input and output'''\n",
    "\n",
    "train_x = train_idxs[:, :-1]\n",
    "print \"x:\", train_x[0]\n",
    "    \n",
    "train_y = train_idxs[:, 1:]#, None] #Keras requires extra dim for y: (batch_size, n_timesteps, 1)\n",
    "print \"y:\", train_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='#6629b2'>Creating the model</font>\n",
    "\n",
    "We'll build an RNN with four layers: \n",
    "1. An input layer that converts word indices into distributed vector representations (embeddings).\n",
    "2. A recurrent hidden layer, the main component of the network. As it observes each word in the story, it integrates the word embedding representation with what it's observed so far to compute a representation (hidden state) of the story at that timepoint. There are a few architectures for this layer - I use the GRU variation, Keras also provides LSTM or just the simple vanilla recurrent layer.\n",
    "3. A second recurrent layer that takes the first as input and operates the same way, since adding more layers generally improves the model.\n",
    "3. A prediction (dense) layer that outputs a probability for each word in the lexicon via the softmax function, where each probability indicates the chance of that word being the next word in the sequence. The model gets feedback during training about what the actual word should be.\n",
    "\n",
    "Of course this is a very simplified explanation of the model, since the focus here is on how to implement it in Keras. For a more thorough explanation of RNNs, see the resources at the bottom.\n",
    "\n",
    "For each layer, we need to specify the number of dimensions (nodes). Since a language model computes a probability distribution each word in the lexicon, the number of dimensions in the prediction layer is equal to the lexicon size. To account for the zeros in the input, we'll add one more dimension so that each word index corresponds to the index of its dimension.\n",
    "\n",
    "When setting up the model, we specify the number of stories in each input batch (batch size) as well as the number of words in each story (n_timesteps). Here, we'll set n_timesteps to be the length of the x and y matrices above.\\** \n",
    "\n",
    "In the recurrent layers, return_sequences=True indicates the hidden state for each word in the story will be returned, as opposed to just the hidden state for the last word. This is necessary for the model to provide an output for each word. The stateful=True setting indicates the RNN will \"remember\" its hidden state until it is explicitly told to forget it via the reset_states() function. This comes into play during the generation stage (or also when n_timesteps is less than the length of x and y\\**), so I will explain this further below.\n",
    "\n",
    "For each word in a story, the prediction layer will output a probability distribution for the next word. To get this sequence of probability distributions rather than just one, we wrap TimeDistributed() class around the Dense layer. The model is trained to maximize the probabilities of the words in the stories, which is what the sparse_categorical_crossentropy loss function does (again, see below for a full explanation of this). \n",
    "\n",
    "One huge benefit of Keras is that it has several optimization algorithms already implemented. I use Adam here, there are several other available including SGD, RMSprop, and Adagrad. You can change other parameters like learning rate and gradient clipping as well.\n",
    "\n",
    "*\\**It is also possible to set n_timesteps to be less than this length and iterate over shorter sequences of words. For example, if we set n_timesteps to 10, the model will slide over each window of 10 words in the stories and perform an update to the parmaters by backpropogating the gradient over these 10 words (for the details of backpropogation, see below). However, we still want the model to \"remember\" everything in the story, not just the previous 10 words, so Keras provides the \"stateful\" option to do this. By setting \"stateful=True\" (here is it False), the hidden state of the model after observing 10 words will be carried over to the next word window. After all the words in a batch of stories have been processed, the reset_states() function can be called to indicate the model should now forget its hidden state and start over with the next batch of stories. You'd need to update the training function below to iterate through a batch of stories by n_timesteps at a time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "def create_rnn(lexicon_size, n_embedding_nodes, n_hidden_nodes, batch_size, n_timesteps):\n",
    "\n",
    "    rnn = Sequential()\n",
    "\n",
    "    #Layer 1\n",
    "    embedding_layer = Embedding(batch_input_shape=(batch_size, n_timesteps),\n",
    "                                input_dim=lexicon_size + 1, #add 1 because word indices start at 1, not 0\n",
    "                                output_dim=n_embedding_nodes, \n",
    "                                mask_zero=True) #mask_zero=True will ignore padding\n",
    "    rnn.add(embedding_layer)\n",
    "\n",
    "    #Layer 2\n",
    "    recurrent_layer1 = GRU(output_dim=n_hidden_nodes,\n",
    "                           return_sequences=True, #return hidden state for each word, not just last one\n",
    "                           stateful=True) #keep track of hidden state while iterating through story\n",
    "    rnn.add(recurrent_layer1)\n",
    "\n",
    "    #Layer 3\n",
    "    recurrent_layer2 = GRU(output_dim=n_hidden_nodes,\n",
    "                           return_sequences=True, \n",
    "                           stateful=True)\n",
    "    rnn.add(recurrent_layer2)\n",
    "\n",
    "    #Layer 4\n",
    "    prediction_layer = TimeDistributed(Dense(lexicon_size + 1,\n",
    "                                       activation=\"softmax\"))\n",
    "    rnn.add(prediction_layer)\n",
    "\n",
    "    #Specify loss function and optimization algorithm, compile model\n",
    "    rnn.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                optimizer='adam')\n",
    "    \n",
    "    return rnn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an RNN with 300 embedding nodes and 500 hidden nodes in each recurrent layer, with a batch size of 20 stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''initialize the RNN'''\n",
    "\n",
    "batch_size = 20\n",
    "rnn = create_rnn(lexicon_size = len(tokenizer.word_index),\n",
    "                 n_embedding_nodes = 300,\n",
    "                 n_hidden_nodes = 500,\n",
    "                 batch_size = batch_size,\n",
    "                 n_timesteps = maxlen - 1) #subtract 1 from maxlen because x and y each have one word less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train the RNN for 10 iterations through the training stories (epochs). The cross-entropy loss indicates how well the model is learning - it should go down with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN on 100 stories for 10 epochs...\n",
      "epoch 1, mean loss: 7.275\n",
      "epoch 2, mean loss: 6.532\n",
      "epoch 3, mean loss: 6.255\n",
      "epoch 4, mean loss: 6.181\n",
      "epoch 5, mean loss: 6.155\n",
      "epoch 6, mean loss: 6.136\n",
      "epoch 7, mean loss: 6.117\n",
      "epoch 8, mean loss: 6.099\n",
      "epoch 9, mean loss: 6.075\n",
      "epoch 10, mean loss: 6.048\n"
     ]
    }
   ],
   "source": [
    "'''train the RNN'''\n",
    "\n",
    "import numpy\n",
    "\n",
    "n_epochs = 10\n",
    "print \"Training RNN on\", len(train_stories), \"stories for\", n_epochs, \"epochs...\"\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []  #track cross-entropy loss during training\n",
    "    for batch_idx in range(0, len(train_stories), batch_size):\n",
    "        batch_x = train_x[batch_idx:batch_idx+batch_size] #get batch for x\n",
    "        batch_y = train_y[batch_idx:batch_idx+batch_size, :, None] #Keras requires y shape:(batch_size, y_length, 1)\n",
    "        loss = rnn.train_on_batch(batch_x, batch_y) #takes a few moments to initialize training\n",
    "        losses.append(loss)\n",
    "        rnn.reset_states() #reset hidden state after each batch\n",
    "    print \"epoch {}, mean loss: {:.3f}\".format(epoch + 1, numpy.mean(losses))\n",
    "    rnn.save('rnn.h5') #save model after each epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Generating sentences</font>\n",
    "\n",
    "Now that the model is trained, it can be used to generate new text. Here, I'll give the model the first four sentences of a new story and have it generate the fifth sentence. To do this, the model reads the initial story in order to produce a probability distribution for the first word in the fifth sentence. We can sample a word from this probability distribution and add it to the story. We repeat this process, each time generating the next word based on the story so far. We stop generating words either when an end-of-sentence token is generated (e.g. \".\", \"!\", or \"?\"). Of course, you can define any stopping criteria (e.g. a specific number of words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded tokenizer with 1519 words in lexicon\n"
     ]
    }
   ],
   "source": [
    "'''load the trained model; in case training was skipped, load all libaries'''\n",
    "\n",
    "import numpy\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.models import load_model\n",
    "\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "    print \"loaded tokenizer with\", len(tokenizer.word_index), \"words in lexicon\"\n",
    "\n",
    "rnn = load_model('rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORY: I was at an outdoor mall. But it was really hot. I was getting very sweaty. So I decided to wash my armpits at an empty public bathroom. \n",
      "[17, 5, 19, 41, 43, 12, 5, 240, 17, 5, 1480, 23, 45, 17, 21, 2, 22, 19, 41] \n",
      "\n",
      "GIVEN ENDING: I was glad nobody walked in.\n"
     ]
    }
   ],
   "source": [
    "'''set up stories used for generation by separating final sentence from first four'''\n",
    "\n",
    "import csv\n",
    "\n",
    "with open('ROCStories_winter2017.csv', 'r') as f: #reload stories in case not already loaded from training\n",
    "    stories = [story[2:] for story in csv.reader(f)] #first two columns are ID and story title, ignore these\n",
    "    stories = stories[1:] #first row is column names, ignore this\n",
    "\n",
    "heldout_endings = [story[-1] for story in stories[-20:]]\n",
    "heldout_stories = [\" \".join(story[:-1]) for story in stories[-20:]]\n",
    "heldout_idxs = tokenizer.texts_to_sequences(heldout_stories)\n",
    "print \"STORY:\", heldout_stories[0], \"\\n\", heldout_idxs[0], \"\\n\"\n",
    "print \"GIVEN ENDING:\", heldout_endings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will generate word indices, so we need to map these numbers back to their corresponding strings. We'll reverse the lexicon dictionary to create a lookup table to get each word from its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: the\n",
      "2: to\n",
      "3: a\n",
      "4: he\n",
      "5: was\n",
      "6: and\n",
      "7: she\n",
      "8: his\n",
      "9: her\n",
      "10: in\n",
      "11: of\n",
      "12: it\n",
      "13: had\n",
      "14: for\n",
      "15: that\n",
      "16: on\n",
      "17: i\n",
      "18: they\n",
      "19: at\n",
      "20: with\n"
     ]
    }
   ],
   "source": [
    "'''create lookup table to get string words from their indices'''\n",
    "\n",
    "lexicon_lookup = {index: word for word, index in tokenizer.word_index.items()}\n",
    "eos_tokens = [\".\", \"?\", \"!\"] #specify which characters should indicate the end of a sentence and halt generation\n",
    "\n",
    "print \"\\n\".join([str(word_idx) + \": \" + word for word_idx, word \n",
    "                 in lexicon_lookup.items()[:20]]) #print a sample of the lookup map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating, the model predicts one word at a time for a given story, but the trained model expects that batch size = 20 and n_timesteps = 63. The easiest thing to do is duplicate the trained model but set the batch size = 1 and n_timesteps = 1. To do this, we just create a new model with these settings and then copy the parameters (weights) of the trained model over the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''duplicate the trained RNN but set batch size = 1 and n_timesteps = 1'''\n",
    "\n",
    "generation_rnn = create_rnn(lexicon_size = len(tokenizer.word_index),\n",
    "                            n_embedding_nodes = 300,\n",
    "                            n_hidden_nodes = 500,\n",
    "                            batch_size = 1,\n",
    "                            n_timesteps = 1)\n",
    "generation_rnn.set_weights(rnn.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate through each story and generate an ending for it. For each story, we need to \"load\" its first four sentences into the model. This can be done using predict_on_batch() function, even though the probability distributions returned by this function are not needed when just reading the story. Because we set stateful=True when creating the RNN, Keras will keep track of the hidden state while iterating through each word, so that's why n_timesteps can be set to 1. Once the ending has been generated, we call reset_states() to clear the hidden state so that the next story can be read.\n",
    "\n",
    "Once the final word in the fourth sentence has been read in a given story, then we use the resulting probability distribution to predict the first word in the fifth sentence. We use numpy.random.choice() to select a word according to its probability. We once again call predict_on_batch() to get a probability distribution for the second word and sample from this distribution. We continue doing this until a word that ends with an end-of-sentence puncutation mark has been selected. Then we decode the generated ending into a string and show it next to the ending that was given in the dataset.\n",
    "\n",
    "You can see that the generated endings are generally not as coherent and well-formed as the human-authored endings, but they do capture some components of the story and they are often more entertaining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORY: I was at an outdoor mall. But it was really hot. I was getting very sweaty. So I decided to wash my armpits at an empty public bathroom.\n",
      "GIVEN ENDING: I was glad nobody walked in.\n",
      "GENERATED ENDING: including on of finally man with the paused karl to buy agreed tie had james furious hours garden recently playing station.\n",
      "\n",
      "\n",
      "STORY: David is a large man. David does not eat healthy things. One evening while eating his steak David starts feeling a pain. He feels a pain in his chest and falls to his knees.\n",
      "GIVEN ENDING: David is having a heart attack his wife calls for help.\n",
      "GENERATED ENDING: bill wear that joking.\n",
      "\n",
      "\n",
      "STORY: Rich was a musician. He made a few hit songs. Rich had a lot of fans who cared about him. He developed cancer.\n",
      "GIVEN ENDING: Eventually the cancer took his life and rich was gone.\n",
      "GENERATED ENDING: off afraid of mocked.\n",
      "\n",
      "\n",
      "STORY: Sam was a star athlete. He ran track at college. There was a big race coming up. Everyone was sure he would win.\n",
      "GIVEN ENDING: Sam got first place.\n",
      "GENERATED ENDING: trying saw down took he aquarium, very and two, electricity become loved every home now him house.\n",
      "\n",
      "\n",
      "STORY: Carl wanted a snack to watch television. He got an opened bag of chips. He sat to watch. When he bit a chip it was squishy.\n",
      "GIVEN ENDING: Carl spat it out and threw the chips away.\n",
      "GENERATED ENDING: in to his her 35 was a turned the to put huff hot fur i about other movement at car, paid extreme his to swim.\n",
      "\n",
      "\n",
      "STORY: I am a member of a clan in an online game. I was asked to help train a new clan. I had never tried to train anyone before. After a few months the new clan was competitive.\n",
      "GIVEN ENDING: I didn't realize I knew that much about the game.\n",
      "GENERATED ENDING: to paused new he of she they it and she huff memory.\n",
      "\n",
      "\n",
      "STORY: I went to the mall today. I had no plans, I just wanted to shop. I visited a new store and found a rug on sale. The rug was weird but it fit my tastes perfectly.\n",
      "GIVEN ENDING: Despite the cost, I bought the rug and I am glad.\n",
      "GENERATED ENDING: picked mud the handkerchief she clothes day it class the for down pump very years corn not had to ride not ridiculous a side.\n",
      "\n",
      "\n",
      "STORY: Sam was a car dealer. He tried to sell a car to a customer. Sam tried to sell it at too high a price. The customer said they would think about it and go back.\n",
      "GIVEN ENDING: They never returned.\n",
      "GENERATED ENDING: santa ride it and knew few out started first in they neighborhood.\n",
      "\n",
      "\n",
      "STORY: Earlier today I was stuck on the highway due to heavy traffic. There was a road rage fight and men were fighting on the highway. There were 3 men and it was 2 against 1. The guy that was alone had a knife.\n",
      "GIVEN ENDING: He then stabbed the other 2 men and everyone ended up at the hospital.\n",
      "GENERATED ENDING: called delicious, work.\n",
      "\n",
      "\n",
      "STORY: Craig was diagnosed with cancer. He decided to fight it. He tried several different approaches and medications. Eventually it went into remission.\n",
      "GIVEN ENDING: Craig and his loved ones were thrilled.\n",
      "GENERATED ENDING: started new the many vegetables mocked.\n",
      "\n",
      "\n",
      "STORY: I decided to clean out all the closets. I got a rag and a trash bag. I went into the closets and picked through the old clothing. After throwing out the old stuff, I started cleaning.\n",
      "GIVEN ENDING: I wiped down all the shelves and doors.\n",
      "GENERATED ENDING: before.\n",
      "\n",
      "\n",
      "STORY: Kate and her friends were in line outside a club in Las Vegas. They waited close to an hour. They were finally able to go inside. And Kate stepped on a step and broke her heel.\n",
      "GIVEN ENDING: Her night was already ruined.\n",
      "GENERATED ENDING: and money.\n",
      "\n",
      "\n",
      "STORY: I was trying to watch my diet. But my family brought home lots of Mexican food. I couldn't resist and dug right in. I ate tons of steak and rice.\n",
      "GIVEN ENDING: But I was glad that I didn't feel bad afterwards.\n",
      "GENERATED ENDING: coach run after death.\n",
      "\n",
      "\n",
      "STORY: Maia is an Army brat, so she moves around a lot. Today, she is starting a new school for the second time this year. She got her schedule and then went to her first class. As she headed to an empty seat, other students watched her.\n",
      "GIVEN ENDING: Maia hated being the new girl at school.\n",
      "GENERATED ENDING: and his the guest tied.\n",
      "\n",
      "\n",
      "STORY: Jane only had one pair of glasses. And she had broken them. Her mother taped them up and sent her to school. When she entered the classroom everyone stared at her.\n",
      "GIVEN ENDING: They all pointed and laughed as she stood wishing she could disappear.\n",
      "GENERATED ENDING: paused very it his but required for to in to gave yell turned new cold there, for hurried for it no it beer that has college know they actions.\n",
      "\n",
      "\n",
      "STORY: The man liked the flavor. He tried to recreate it at home. He could not get the flavor right. He asked the owner of the recipe for help.\n",
      "GIVEN ENDING: The owner of the flavor sold him the recipe.\n",
      "GENERATED ENDING: marveled her out footwear.\n",
      "\n",
      "\n",
      "STORY: After my friend's dad's funeral, I got in trouble. The principal said I wasn't allowed to leave school that day. He found out I had my friend sign me out. He told me I was getting detention.\n",
      "GIVEN ENDING: I skipped detention all week.\n",
      "GENERATED ENDING: come his soon went driver.\n",
      "\n",
      "\n",
      "STORY: Janice was out exercising for her big soccer game. She was doing some drills with her legs. While working out and exercising she slips on the grass. She falls down and uses her wrist to break her fall.\n",
      "GIVEN ENDING: She breaks her wrist in the process and goes to the hospital.\n",
      "GENERATED ENDING: nap didn't a cheap.\n",
      "\n",
      "\n",
      "STORY: Jamie is an american girl. Jamie wants to get married to a mexican man. Her family assumes it's because the man wants a green card. Jamie insist that she is marrying him out of love.\n",
      "GIVEN ENDING: Jamie gets married and they spent the rest of their lives together.\n",
      "GENERATED ENDING: he movie be because new was hours a at house.\n",
      "\n",
      "\n",
      "STORY: The orange fell from the tree. It hit a girl on the head. The girl looked up at the tree. Another orange fell from the tree.\n",
      "GIVEN ENDING: That orange broke her nose.\n",
      "GENERATED ENDING: behind dad so brother he couldn't went the the his on bold, graduation went up lapped dog see dog.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''use RNN to generate new endings for stories'''\n",
    "\n",
    "for story, story_idxs, ending in zip(heldout_stories, heldout_idxs, heldout_endings):\n",
    "    print \"STORY:\", story\n",
    "    print \"GIVEN ENDING:\", ending\n",
    "    \n",
    "    generated_ending = []\n",
    "    \n",
    "    story_idxs = numpy.array(story_idxs)[None] #format story with shape (1, length)\n",
    "    \n",
    "    for step_idx in range(story_idxs.shape[-1]):\n",
    "        p_next_word = generation_rnn.predict_on_batch(story_idxs[:, step_idx])[0,-1] #load the story; input shape will be (1, 1)\n",
    "\n",
    "    while not generated_ending or lexicon_lookup[next_word][-1] not in eos_tokens: #now start predicting new words\n",
    "        next_word = numpy.random.choice(a=p_next_word.shape[-1], p=p_next_word)\n",
    "        generated_ending.append(next_word)\n",
    "        p_next_word = generation_rnn.predict_on_batch(numpy.array(next_word)[None,None])[0,-1]\n",
    "    \n",
    "    generation_rnn.reset_states() #reset hidden state after generating ending\n",
    "    \n",
    "    generated_ending = \" \".join([lexicon_lookup[word] \n",
    "                                 for word in generated_ending]) #decode from numbers back into words\n",
    "    print \"GENERATED ENDING:\", generated_ending\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Conclusion</font>\n",
    "\n",
    "Because it's an amusing task and illustrates the power of RNNs, there are now many tutorials online about text generation with RNNs. This one shows one way to do it in Keras with batch training when the length of the sequences is variable. This also demonstrates how you can input existing text into the RNN and generate a continuation of it.\n",
    "\n",
    "There are many ways this language model can be made to be more sophisticated. Here's a few interesting papers from the NLP community that innovate this basic model for different generation tasks:\n",
    "\n",
    "*Recipe generation:* [Globally Coherent Text Generation with Neural Checklist Models](https://homes.cs.washington.edu/~yejin/Papers/emnlp16_neuralchecklist.pdf). Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\n",
    "\n",
    "*Emotional text generation:* [Affect-LM: A Neural Language Model for Customizable Affective Text Generation](https://arxiv.org/pdf/1704.06851.pdf). Sayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency, Stefan Scherer. Annual Meeting of the Association for Computational Linguistics (ACL), 2017.\n",
    "\n",
    "*Poetry generation:* [Generating Topical Poetry](https://www.isi.edu/natural-language/mt/emnlp16-poetry.pdf). Marjan Ghazvininejad, Xing Shi, Yejin Choi, and Kevin Knight. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\n",
    "\n",
    "*Dialogue generation:* [A Neural Network Approach to Context-Sensitive Generation of Conversational Responses](http://www-etud.iro.umontreal.ca/~sordonia/pdf/naacl15.pdf). Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie1, Jianfeng Gao, Bill Dolan. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Helpful resources about RNNs for text processing</font>\n",
    "\n",
    "Among the [Theano tutorials](http://deeplearning.net/tutorial/) mentioned above, there are two specifically on RNNs for NLP: [semantic parsing](http://deeplearning.net/tutorial/rnnslu.html#rnnslu) and [sentiment analysis](http://deeplearning.net/tutorial/lstm.html#lstm)\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (same model as shown here, with raw Python code) \n",
    "\n",
    "TensorFlow also has an RNN language model [tutorial](https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html) using the Penn Treebank dataset\n",
    "\n",
    "This [explanation](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) of how LSTMs work and why they are better than plain RNNs (this explanation also applies to the GRU used here)\n",
    "\n",
    "Another [tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) that documents well both the theory of RNNs and their implementation in Python (and if you care to implement the details of the stochastic gradient descent and backprogation through time algorithms, this is very informative)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
