{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#6629b2'>Generating text with recurrent neural networks using Keras</font>\n",
    "by Melissa Roemmele, 7/17/17, roemmele @ usc.edu\n",
    "\n",
    "## <font color='#6629b2'>Overview</font>\n",
    "\n",
    "I am going to show how to build a recurrent neural network (RNN) language model that learns the relation between words in text, using the Keras library for machine learning. I will then show how this model can be used for text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Recurrent Neural Networks (RNNs)</font>\n",
    "\n",
    "RNNs are a general framework for modeling sequence data and are particularly useful for natural langugage processing tasks. Here an RNN will be used as a language model, which can predict which word is likely to occur next in a text given the words before it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Keras</font>\n",
    "\n",
    "[Keras](https://keras.io/) is a Python deep learning framework that lets you quickly put together neural network models with a minimal amount of code. It can be run on top of [Theano](http://deeplearning.net/software/theano/) or [Tensor Flow](https://www.tensorflow.org/) without you needing to know either of these underlying frameworks. It provides implementations of several of the layer architectures, objective functions, and optimization algorithms you need for building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Dataset</font>\n",
    "\n",
    "My research is on story generation, so I've selected a dataset of stories as the text to be modeled by the RNN. They come from the [ROCStories](http://cs.rochester.edu/nlp/rocstories/) dataset, which consists of thousands of five-sentence stories about everyday life events. Here the model will observe all five sentences in each story. Then we'll use the trained model to generate the final sentence in a set of stories not observed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''load the dataset'''\n",
    "import csv\n",
    "\n",
    "with open('roc_train_stories97027.csv', 'r') as f:\n",
    "    stories = [story for story in csv.reader(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan's parents were overweight. Dan was overweight as well. The doctors told his parents it was unhealthy. His parents understood and decided to make a change. They got themselves and Dan on a diet.\n",
      "\n",
      "Carrie had just learned how to ride a bike. She didn't have a bike of her own. Carrie would sneak rides on her sister's bike. She got nervous on a hill and crashed into a wall. The bike frame bent and Carrie got a deep gash on her leg.\n",
      "\n",
      "Morgan enjoyed long walks on the beach. She and her boyfriend decided to go for a long walk. After walking for over a mile, something happened. Morgan decided to propose to her boyfriend. Her boyfriend was upset he didn't propose to her first.\n",
      "\n",
      "Jane was working at a diner. Suddenly, a customer barged up to the counter. He began yelling about how long his food was taking. Jane didn't know how to react. Luckily, her coworker intervened and calmed the man down.\n",
      "\n",
      "I was talking to my crush today. She continued to complain about guys flirting with her. I decided to agree with what she says and listened to her patiently. After I got home, I got a text from her. She asked if we can hang out tomorrow.\n"
     ]
    }
   ],
   "source": [
    "'''designate a set of training stories'''\n",
    "\n",
    "train_stories = stories[:500]\n",
    "train_stories = [\" \".join(story) for story in train_stories] #concatenate sentences in each story into one long string\n",
    "print \"\\n\\n\".join(train_stories[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Preparing the data</font>\n",
    "\n",
    "The model we'll create is a word-based language model, which means each input unit is a single word (some language models learn subword units like characters). \n",
    "\n",
    "So first we need to tokenize each of the stories into (lowercased) individual words. I'll use Keras' built-in tokenizer here for convenience, but typically I like to use [spacy](https://spacy.io/), a fast and user-friendly library that performs various language processing tasks. \n",
    "\n",
    "A note: Keras' tokenizer does not do the same linguistic processing to separate punctuation from words, for instance, which should be their own tokens. You can see this below from words that end in punctuation like \".\" or \",\".\n",
    "\n",
    "We need to assemble a lexicon (aka vocabulary) of words that the model needs to know. Thus, each tokenized word in the stories is added to the lexicon. We use the fit_on_texts() function to map each word in the stories to a numerical index. When working with large datasets it's common to filter all words occurring less than a certain number of times, and replace them with some generic \"UNKNOWN\" token. Here, because this dataset is small, every word encountered in the stories is added to the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raining: 1139\n",
      "ever.: 1140\n",
      "limited: 1929\n",
      "shouted.: 1930\n",
      "better.: 612\n",
      "jane's: 1931\n",
      "foul: 1141\n",
      "four: 811\n",
      "obstruction: 1932\n",
      "woods: 1933\n",
      "sleep: 812\n",
      "ocean.: 1142\n",
      "hyped.: 1934\n",
      "captain: 1143\n",
      "party.: 613\n",
      "party,: 1935\n",
      "marching: 1936\n",
      "up.: 164\n",
      "belonged.: 1937\n",
      "up,: 813\n"
     ]
    }
   ],
   "source": [
    "'''make the lexicon'''\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(lower=True, filters='')\n",
    "tokenizer.fit_on_texts(train_stories) #split stories into words, assign number to each unique word\n",
    "print \"\\n\".join([word + \": \" + str(word_idx) for word, word_idx \n",
    "                 in tokenizer.word_index.items()[:20]]) #print a sample of the dictionary\n",
    "\n",
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as f: #save the tokenizer\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dan's parents were overweight. Dan was overweight as well. The doctors told his parents it was unhealthy. His parents understood and decided to make a change. They got themselves and Dan on a diet. \n",
      "\n",
      "[1986, 159, 33, 4116, 176, 4, 3784, 34, 517, 1, 3573, 57, 9, 159, 15, 4, 3633, 9, 159, 3840, 6, 27, 2, 69, 3, 503, 18, 32, 4070, 6, 176, 16, 3, 1879]\n"
     ]
    }
   ],
   "source": [
    "'''convert each story from text to numbers'''\n",
    "\n",
    "train_idxs = tokenizer.texts_to_sequences(train_stories) #transform each word to its numerical index in lexicon\n",
    "print train_stories[0], \"\\n\"\n",
    "print train_idxs[0] #show example of encoded story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Creating a matrix</font>\n",
    "\n",
    "Finally, we need to put all the training stories into a single matrix, where each row is a story and each column is a word index in that story. This enables the model to process the stories in batches as opposed to one at a time, which significantly speeds up training. However, each story has a different number of words. So we create a padded matrix equal to the length on the longest story in the training set. For all stories with fewer words, we prepend the row with zeros representing an empty word position. Then we can actually tell Keras to ignore these zeros during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix length: 66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1986,\n",
       "        159,   33, 4116,  176,    4, 3784,   34,  517,    1, 3573,   57,\n",
       "          9,  159,   15,    4, 3633,    9,  159, 3840,    6,   27,    2,\n",
       "         69,    3,  503,   18,   32, 4070,    6,  176,   16,    3, 1879], dtype=int32)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''create a padded matrix of stories'''\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = max([len(story) for story in train_idxs]) # get length of longest story\n",
    "# maxlen = int(numpy.ceil(maxlen * 1. / 10))\\\n",
    "#                                 * 10 + 1 #for reasons that will become clear below, round up to nearest 10 and add 1\n",
    "print \"matrix length:\", maxlen\n",
    "\n",
    "train_idxs = pad_sequences(train_idxs, maxlen=maxlen) #keras provides convenient padding function\n",
    "train_idxs[0] #same example story as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Defining the input and output</font>\n",
    "\n",
    "In an RNN language model, the data is set up so that each word in the text is mapped to the word that follows it. In a given story, for each input word x[idx], the output label y[idx] is just x[idx+1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 1986  159   33 4116  176    4 3784   34  517    1 3573   57    9\n",
      "  159   15    4 3633    9  159 3840    6   27    2   69    3  503   18   32\n",
      " 4070    6  176   16    3]\n",
      "y: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0 1986  159   33 4116  176    4 3784   34  517    1 3573   57    9  159\n",
      "   15    4 3633    9  159 3840    6   27    2   69    3  503   18   32 4070\n",
      "    6  176   16    3 1879]\n"
     ]
    }
   ],
   "source": [
    "'''set up the model input and output'''\n",
    "\n",
    "train_x = train_idxs[:, :-1]\n",
    "print \"x:\", train_x[0]\n",
    "    \n",
    "train_y = train_idxs[:, 1:]#, None] #Keras requires extra dim for y: (batch_size, n_timesteps, 1)\n",
    "print \"y:\", train_y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='#6629b2'>Creating the model</font>\n",
    "\n",
    "We'll build an RNN with four layers: \n",
    "1. An input layer that converts word indices into distributed vector representations (embeddings).\n",
    "2. A recurrent hidden layer, the main component of the network. As it observes each word in the story, it integrates the word embedding representation with what it's observed so far to compute a representation (hidden state) of the story at that timepoint. There are a few architectures for this layer - I use the GRU variation, Keras also provides LSTM or just the simple vanilla recurrent layer.\n",
    "3. A second recurrent layer that takes the first as input and operates the same way, since adding more layers generally improves the model.\n",
    "3. A prediction (dense) layer that outputs a probability for each word in the lexicon, where each probability indicates the chance of that word being the next word in the sequence. The model gets feedback during training about what the actual word should be.\n",
    "\n",
    "Of course this is a very simplified explanation of the model, since the focus here is on how to implement it in Keras. For a more thorough explanation of RNNs, see the resources at the bottom.\n",
    "\n",
    "For each layer, we need to specify the number of dimensions (nodes). Since a language model computes a probability distribution each word in the lexicon, the number of dimensions in the prediction layer is equal to the lexicon size. To account for the zeros in the input, we'll add one more dimension so that each word index corresponds to the index of its dimension.\n",
    "\n",
    "When setting up the model, we specify the number of stories in each input batch (batch size) as well as the number of words observed at a time for each story in a batch (n_timesteps). For example, if n_timesteps is 10, the model will slide over each window of 10 words in the stories and perform an update to the parmaters by backpropogating the gradient over these 10 words (for the details of backpropogation, see below). However, we still want the model to \"remember\" everything in the story, not just the previous 10 words, so Keras provides the \"stateful\" option to do this. By setting \"stateful=True\", the hidden state of the model after observing 10 words will be carried over to the next word window. After all the words in a batch of stories have been processed, the reset_states() function can be called to indicate the model should now forget its hidden state and start over with the next batch of stories.\n",
    "\n",
    "For each word in a story, the prediction layer will output a probability distribution for the next word. To get this sequence of probability distributions rather than just one, we wrap TimeDistributed() class around the Dense layer. The model is trained to maximize the probabilities of the words in the stories, which is what the sparse_categorical_crossentropy loss function does (again, see below for a full explanation of this). \n",
    "\n",
    "One huge benefit of Keras is that it has several optimization algorithms already implemented. I use Adam here, there are several other available including SGD, RMSprop, and Adagrad. You can change other parameters like learning rate and gradient clipping as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "def create_rnn(lexicon_size, n_embedding_nodes, n_hidden_nodes, batch_size, n_timesteps):\n",
    "\n",
    "    rnn = Sequential()\n",
    "\n",
    "    #Layer 1\n",
    "    embedding_layer = Embedding(batch_input_shape=(batch_size, n_timesteps),\n",
    "                                input_dim=lexicon_size + 1, #add 1 because word indices start at 1, not 0\n",
    "                                output_dim=n_embedding_nodes, \n",
    "                                mask_zero=True) #mask_zero=True will ignore padding\n",
    "    rnn.add(embedding_layer)\n",
    "\n",
    "    #Layer 2\n",
    "    recurrent_layer1 = GRU(output_dim=n_hidden_nodes,\n",
    "                           return_sequences=True, #return hidden state for each word, not just last one\n",
    "                           stateful=True) #keep track of hidden state while iterating through story\n",
    "    rnn.add(recurrent_layer1)\n",
    "\n",
    "    #Layer 3\n",
    "    recurrent_layer2 = GRU(output_dim=n_hidden_nodes,\n",
    "                           return_sequences=True, \n",
    "                           stateful=True)\n",
    "    rnn.add(recurrent_layer2)\n",
    "\n",
    "    #Layer 4\n",
    "    prediction_layer = TimeDistributed(Dense(lexicon_size + 1,\n",
    "                                       activation=\"softmax\"))\n",
    "    rnn.add(prediction_layer)\n",
    "\n",
    "    #Specify loss function and optimization algorithm, compile model\n",
    "    rnn.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "                optimizer='adam')\n",
    "    \n",
    "    return rnn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an RNN with 300 embedding nodes and 500 hidden nodes in each recurrent layer. It will process 10 words (n_timesteps) at a time in batches of 20 stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''initialize the RNN'''\n",
    "\n",
    "batch_size = 20\n",
    "rnn = create_rnn(lexicon_size = len(tokenizer.word_index),\n",
    "                 n_embedding_nodes = 300,\n",
    "                 n_hidden_nodes = 500,\n",
    "                 batch_size = batch_size,\n",
    "                 n_timesteps = maxlen - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train the RNN for 10 iterations through the training stories (epochs). The cross-entropy loss indicates how well the model is learning - it should go down with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN on 500 stories for 50 epochs...\n",
      "epoch 1, mean loss: 7.490\n",
      "epoch 2, mean loss: 6.807\n",
      "epoch 3, mean loss: 6.731\n",
      "epoch 4, mean loss: 6.680\n",
      "epoch 5, mean loss: 6.662\n",
      "epoch 6, mean loss: 6.649\n",
      "epoch 7, mean loss: 6.637\n",
      "epoch 8, mean loss: 6.621\n",
      "epoch 9, mean loss: 6.594\n",
      "epoch 10, mean loss: 6.518\n",
      "epoch 11, mean loss: 6.386\n",
      "epoch 12, mean loss: 6.271\n",
      "epoch 13, mean loss: 6.175\n",
      "epoch 14, mean loss: 6.064\n",
      "epoch 15, mean loss: 5.960\n",
      "epoch 16, mean loss: 5.853\n",
      "epoch 17, mean loss: 5.750\n",
      "epoch 18, mean loss: 5.622\n",
      "epoch 19, mean loss: 5.515\n",
      "epoch 20, mean loss: 5.358\n",
      "epoch 21, mean loss: 5.209\n",
      "epoch 22, mean loss: 5.090\n",
      "epoch 23, mean loss: 4.978\n",
      "epoch 24, mean loss: 4.854\n",
      "epoch 25, mean loss: 4.727\n",
      "epoch 26, mean loss: 4.666\n",
      "epoch 27, mean loss: 4.580\n",
      "epoch 28, mean loss: 4.442\n",
      "epoch 29, mean loss: 4.291\n",
      "epoch 30, mean loss: 4.113\n",
      "epoch 31, mean loss: 3.954\n",
      "epoch 32, mean loss: 3.824\n",
      "epoch 33, mean loss: 3.693\n",
      "epoch 34, mean loss: 3.558\n",
      "epoch 35, mean loss: 3.431\n",
      "epoch 36, mean loss: 3.300\n",
      "epoch 37, mean loss: 3.182\n",
      "epoch 38, mean loss: 3.054\n",
      "epoch 39, mean loss: 2.906\n",
      "epoch 40, mean loss: 2.776\n",
      "epoch 41, mean loss: 2.643\n",
      "epoch 42, mean loss: 2.508\n",
      "epoch 43, mean loss: 2.415\n",
      "epoch 44, mean loss: 2.316\n",
      "epoch 45, mean loss: 2.224\n",
      "epoch 46, mean loss: 2.095\n",
      "epoch 47, mean loss: 1.976\n",
      "epoch 48, mean loss: 1.882\n",
      "epoch 49, mean loss: 1.784\n",
      "epoch 50, mean loss: 1.680\n"
     ]
    }
   ],
   "source": [
    "'''train the RNN'''\n",
    "\n",
    "import numpy\n",
    "\n",
    "n_epochs = 10\n",
    "print \"Training RNN on\", len(train_stories), \"stories for\", n_epochs, \"epochs...\"\n",
    "for epoch in range(n_epochs):\n",
    "    #import pdb;pdb.set_trace()\n",
    "    losses = []  #track cross-entropy loss during training\n",
    "    for batch_idx in range(0, len(train_stories), batch_size):\n",
    "        batch_x = train_x[batch_idx:batch_idx+batch_size] #get batch for x\n",
    "        batch_y = train_y[batch_idx:batch_idx+batch_size, :, None] #get batch for y, Keras requires extra final dimension\n",
    "        loss = rnn.train_on_batch(batch_x, batch_y) #takes a few moments to initialize training\n",
    "        losses.append(loss)\n",
    "        rnn.reset_states() #reset hidden state after each batch\n",
    "    print \"epoch {}, mean loss: {:.3f}\".format(epoch + 1, numpy.mean(losses))\n",
    "    rnn.save('rnn.h5') #save model after each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''load the trained model'''\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "with open('tokenizer_96000_stories.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "    print \"loaded tokenizer with\", len(tokenizer.word_index), \"words in lexicon\"\n",
    "\n",
    "rnn = load_model('rnn_96000_stories.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Generating sentences</font>\n",
    "\n",
    "Now that the model is trained, it can be used to generate new text. Here, I'll give the model the first four sentences of a new story and have it generate the fifth sentence. To do this, the model reads the initial story in order to produce a probability distribution for the first word in the fifth sentence. We can sample a word from this probability distribution and add it to the story. We repeat this process, each time generating the next word based on the story so far. We stop generating words either when an end-of-sentence token is generated (e.g. \".\", \"!\", or \"?\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORY: Dan's parents were overweight. Dan was overweight as well. The doctors told his parents it was unhealthy. His parents understood and decided to make a change. \n",
      "[1986, 159, 33, 4116, 176, 4, 3784, 34, 517, 1, 3573, 57, 9, 159, 15, 4, 3633, 9, 159, 3840, 6, 27, 2, 69, 3, 503] \n",
      "\n",
      "\n",
      "GIVEN ENDING: They got themselves and Dan on a diet.\n"
     ]
    }
   ],
   "source": [
    "'''set up stories used for generation by separating final sentence from first four'''\n",
    "\n",
    "heldout_endings = [story[-1] for story in stories[-20:]]\n",
    "heldout_stories = [\" \".join(story[:-1]) for story in stories[-20:]]\n",
    "heldout_idxs = tokenizer.texts_to_sequences(heldout_stories)\n",
    "print \"STORY:\", heldout_stories[0], \"\\n\", heldout_idxs[0], \"\\n\"\n",
    "print \"GIVEN ENDING:\", heldout_endings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating, the model predicts one word at a time for a given story, but the trained model expects that batch size = 20 and n_timesteps = 70. The easiest thing to do is duplicate the trained model but set the batch size = 1 and n_timesteps = 1. To do this, we just create a new model with these settings and then copy the parameters (weights) of the trained model over the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''duplicate the trained RNN but set batch size = 1 and n_timesteps = 1'''\n",
    "\n",
    "generation_rnn = create_rnn(lexicon_size = len(tokenizer.word_index),\n",
    "                            n_embedding_nodes = 300,\n",
    "                            n_hidden_nodes = 500,\n",
    "                            batch_size = 1,\n",
    "                            n_timesteps = 1)\n",
    "generation_rnn.set_weights(rnn.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model will generate word indices, so we need to map these numbers back to their corresponding strings. We'll reverse the lexicon dictionary to create a lookup table to get each word from its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: the\n",
      "2: to\n",
      "3: a\n",
      "4: was\n",
      "5: he\n",
      "6: and\n",
      "7: she\n",
      "8: her\n",
      "9: his\n",
      "10: i\n",
      "11: for\n",
      "12: in\n",
      "13: of\n",
      "14: had\n",
      "15: it\n",
      "16: on\n",
      "17: my\n",
      "18: they\n",
      "19: with\n",
      "20: that\n"
     ]
    }
   ],
   "source": [
    "'''create lookup table to get string words from their indices'''\n",
    "\n",
    "lexicon_lookup = {index: word for word, index in tokenizer.word_index.items()}\n",
    "eos_tokens = [\".\", \"?\", \"!\"] #specify which characters should indicate the end of a sentence and halt generation\n",
    "\n",
    "print \"\\n\".join([str(word_idx) + \": \" + word for word_idx, word \n",
    "                 in lexicon_lookup.items()[:20]]) #print a sample of the lookup map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORY: Dan's parents were overweight. Dan was overweight as well. The doctors told his parents it was unhealthy. His parents understood and decided to make a change.\n",
      "GIVEN ENDING: They got themselves and Dan on a diet.\n",
      "GENERATED ENDING: they concert.\n",
      "\n",
      "\n",
      "STORY: Carrie had just learned how to ride a bike. She didn't have a bike of her own. Carrie would sneak rides on her sister's bike. She got nervous on a hill and crashed into a wall.\n",
      "GIVEN ENDING: The bike frame bent and Carrie got a deep gash on her leg.\n",
      "GENERATED ENDING: carrie agreed and positive over her side but time she would sneak but she bike into the bike for the wall.\n",
      "\n",
      "\n",
      "STORY: Morgan enjoyed long walks on the beach. She and her boyfriend decided to go for a long walk. After walking for over a mile, something happened. Morgan decided to propose to her boyfriend.\n",
      "GIVEN ENDING: Her boyfriend was upset he didn't propose to her first.\n",
      "GENERATED ENDING: the boyfriend would laughed for her with her morning searching something that treatment was quite cooked after all her lot of months and kept there.\n",
      "\n",
      "\n",
      "STORY: Jane was working at a diner. Suddenly, a customer barged up to the counter. He began yelling about how long his food was taking. Jane didn't know how to react.\n",
      "GIVEN ENDING: Luckily, her coworker intervened and calmed the man down.\n",
      "GENERATED ENDING: dollars.\n",
      "\n",
      "\n",
      "STORY: I was talking to my crush today. She continued to complain about guys flirting with her. I decided to agree with what she says and listened to her patiently. After I got home, I got a text from her.\n",
      "GIVEN ENDING: She asked if we can hang out tomorrow.\n",
      "GENERATED ENDING: she asked me after our program.\n",
      "\n",
      "\n",
      "STORY: Frank had been drinking beer. He got a call from his girlfriend, asking where he was. Frank suddenly realized he had a date that night. Since Frank was already a bit drunk, he could not drive.\n",
      "GIVEN ENDING: Frank spent the rest of the night drinking more beers.\n",
      "GENERATED ENDING: frank drunk, he takes the zippy we would not get with a rest of the little so so he could wait to wait on arrested.\n",
      "\n",
      "\n",
      "STORY: Dave was in the Bahamas on vacation. He decided to go snorkeling on his second day. While snorkeling, he saw a cave up ahead. He went into the cave, and he was terrified when he found a shark!\n",
      "GIVEN ENDING: Dave swam away as fast as he could, but the shark caught and ate Dave.\n",
      "GENERATED ENDING: dave approached his music.\n",
      "\n",
      "\n",
      "STORY: Sunny enjoyed going to the beach. As she stepped out of her car, she realized she forgot something. It was quite sunny and she forgot her sunglasses. Sunny got back into her car and heading towards the mall.\n",
      "GIVEN ENDING: Sunny found some sunglasses and headed back to the beach.\n",
      "GENERATED ENDING: again.\n",
      "\n",
      "\n",
      "STORY: Sally was happy when her widowed mom found a new man. She discovered her siblings didn't feel the same. Sally flew to visit her mom and her mom's new husband. Although her mom was obviously in love, he was nothing like her dad.\n",
      "GIVEN ENDING: Sally went home and wondered about her parents' marriage.\n",
      "GENERATED ENDING: where her friends and flew off money them more made for love, she made them and feel two dad.\n",
      "\n",
      "\n",
      "STORY: Dan hit his golf ball and watched it go. The ball bounced on the grass and into the sand trap. Dan pretended that his ball actually landed on the green. His friends were not paying attention so they believed him.\n",
      "GIVEN ENDING: Dan snuck a ball on the green and made his putt from 10 feet.\n",
      "GENERATED ENDING: dan tried a empty friends though.\n",
      "\n",
      "\n",
      "STORY: Josh had a parrot that talked. He brought his parrot to school. During show and tell, Josh's parrot said a bad word. The teacher told Joshua not to bring his bird again.\n",
      "GIVEN ENDING: When Josh got home, he was grounded.\n",
      "GENERATED ENDING: when the women went out to cancel his friends damaged.\n",
      "\n",
      "\n",
      "STORY: Hal was walking his dog one morning. A cat ran across their path. Hal's dog strained so hard, the leash broke! He chased the cat for several minutes.\n",
      "GIVEN ENDING: Finally Hal lured him back to his side.\n",
      "GENERATED ENDING: declining the parking not causing their fruit.\n",
      "\n",
      "\n",
      "STORY: Brenda was in love with her boyfriend Maxwell. He was a successful artist with a promising future. Maxwell told Brenda he needed to talk to her. She thought he'd propose but he wanted to break up.\n",
      "GIVEN ENDING: Brenda walked away and now she is the saddest girl out of everyone.\n",
      "GENERATED ENDING: she was only bad.\n",
      "\n",
      "\n",
      "STORY: Yanice opened the fridge and found nothing to eat. However, there were leftovers. She mixed it up in an attempt to make lunch. Since the place needed meat, she also fried and eggs.\n",
      "GIVEN ENDING: She ended up enjoying the meal.\n",
      "GENERATED ENDING: she went to the food and worried she was getting tears.\n",
      "\n",
      "\n",
      "STORY: I saw my friend Joe sitting in lobby today. I kept him company, as he is a lonely old man. He told me he had just listened to Beethoven's Ninth. I talked to him for an hour.\n",
      "GIVEN ENDING: I left him in the lobby and told him I would see him soon.\n",
      "GENERATED ENDING: i and passionate out spaghetti i crashed him so several entry winter project.\n",
      "\n",
      "\n",
      "STORY: Twas the night after the first day of junior high. Amy and her friend Beth were on the phone. They had a lot to catch up on. Amy listened patiently as Beth told her about her day.\n",
      "GIVEN ENDING: She wanted to go 2nd because she knew hers was the better day.\n",
      "GENERATED ENDING: but evening, why proud.\n",
      "\n",
      "\n",
      "STORY: I knew of a young man who won the lottery. He used to ride lawn mowers. After he won, he went on to using drugs. He blew a lot of money.\n",
      "GIVEN ENDING: Eventually his winnings were revoked after a dui.\n",
      "GENERATED ENDING: and then one coefficients trip.\n",
      "\n",
      "\n",
      "STORY: A die hard shopper was waiting in the long line outside. It was miserably cold. The shopper saw a homeless man shivering in the alleyway. He gave up his place in the line and brought a gift back from his car.\n",
      "GIVEN ENDING: The shopper gave the homeless man a nice warm blanket.\n",
      "GENERATED ENDING: the shopper gave the shopper gave it next dan shivering in his alleyway.\n",
      "\n",
      "\n",
      "STORY: Jeff invited his friends over to play board games on Saturday night. They arrived at his house early that evening. The six of them sat around a big table. They took turns deciding which game to play.\n",
      "GIVEN ENDING: They spent six hours playing different board games.\n",
      "GENERATED ENDING: they hoped it which playing at different work.\n",
      "\n",
      "\n",
      "STORY: Chuck reclined on the back porch as he sipped his morning coffee. Today he would finally screen in this back porch. He gathered his tools and material supplies. He labored all day to finish the job.\n",
      "GIVEN ENDING: That night he snuggled with his wife on the bug free porch.\n",
      "GENERATED ENDING: and snuggled he liked them for his car.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''use RNN to generate new endings for stories'''\n",
    "\n",
    "for story, story_idxs, ending in zip(heldout_stories, heldout_idxs, heldout_endings):\n",
    "    #import pdb;pdb.set_trace()\n",
    "    print \"STORY:\", story\n",
    "    print \"GIVEN ENDING:\", ending\n",
    "    \n",
    "    generated_ending = []\n",
    "    \n",
    "    story_idxs = numpy.array(story_idxs)[None] #format story with shape (1, length)\n",
    "    \n",
    "    for step_idx in range(story_idxs.shape[-1]):\n",
    "        p_next_word = generation_rnn.predict_on_batch(story_idxs[:, step_idx])[0,-1] #input shape will be (1, 1)\n",
    "\n",
    "    #now start predicting new words\n",
    "    while not generated_ending or lexicon_lookup[next_word][-1] not in eos_tokens:\n",
    "        next_word = numpy.random.choice(a=p_next_word.shape[-1], p=p_next_word)\n",
    "        generated_ending.append(next_word)\n",
    "        p_next_word = generation_rnn.predict_on_batch(numpy.array(next_word)[None,None])[0,-1]\n",
    "    \n",
    "    generation_rnn.reset_states() #reset hidden state after generating ending\n",
    "    \n",
    "    generated_ending = \" \".join([lexicon_lookup[word] \n",
    "                                 for word in generated_ending]) #decode from numbers back into words\n",
    "    print \"GENERATED ENDING:\", generated_ending\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Helpful resources about RNNs for text processing</font>\n",
    "\n",
    "Among the [Theano tutorials](http://deeplearning.net/tutorial/) mentioned above, there are two specifically on RNNs for NLP: [semantic parsing](http://deeplearning.net/tutorial/rnnslu.html#rnnslu) and [sentiment analysis](http://deeplearning.net/tutorial/lstm.html#lstm)\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (same model as shown here, with raw Python code) \n",
    "\n",
    "TensorFlow also has an RNN language model [tutorial](https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html) using the Penn Treebank dataset\n",
    "\n",
    "This [explanation](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) of how LSTMs work and why they are better than plain RNNs (this explanation also applies to the GRU used here)\n",
    "\n",
    "Another [tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) that documents well both the theory of RNNs and their implementation in Python (and if you care to implement the details of the stochastic gradient descent and backprogation through time algorithms, this is very informative)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
