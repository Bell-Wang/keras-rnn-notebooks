{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#6629b2'>Language modeling with recurrent neural networks using Keras</font>\n",
    "### https://github.com/roemmele/keras-rnn-demo/language-modeling\n",
    "by Melissa Roemmele, 10/30/17, roemmele @ usc.edu\n",
    "\n",
    "## <font color='#6629b2'>Overview</font>\n",
    "\n",
    "I am going to show how to build a recurrent neural network (RNN) language model that learns the relation between words in text, using the Keras library for machine learning. I will then show how this model can be used for text generation.\n",
    "\n",
    "### <font color='#6629b2'>Language Modeling</font>\n",
    "\n",
    "A language model is a model of the probability of word sequences. These models are useful for a variety of tasks, such as ones that require selecting from a set of candiate outputs as in speech recognition or machine translation, for example. Here, I'll show how a language model can be used to generate the endings of stories. Language generation is a difficult research problem which is generally addressed by more complex models than the one shown here.\n",
    "\n",
    "Traditionally, the most well-known approach to language modeling relies on n-grams. The limitation of n-gram language models is that they only explitly model the probability of a sequence of *n* words. In contrast, RNNs can model longer sequences and thus typically are better at predicting which words will appear in a sequence. See the [chapter in Jurafsky & Martin's *Speech and Language Processing*](https://web.stanford.edu/~jurafsky/slp3/4.pdf) to learn more about traditional approaches to language modeling. \n",
    "\n",
    "### <font color='#6629b2'>Recurrent Neural Networks (RNNs)</font>\n",
    "\n",
    "RNNs are a general framework for modeling sequence data and are particularly useful for natural language processing tasks. At a high level, RNN encode sequences via a set of parameters (weights) that are optimized to predict some output variable. The focus of this tutorial is on the code needed to assemble a model in Keras. For a more general introduction to RNNs, see the resources at the bottom. Here an RNN will be used as a language model, which can predict which word is likely to occur next in a text given the words before it.\n",
    "\n",
    "### <font color='#6629b2'>Keras</font>\n",
    "\n",
    "[Keras](https://keras.io/) is a Python deep learning framework that lets you quickly put together neural network models with a minimal amount of code. It can be run on top of [Theano](http://deeplearning.net/software/theano/) or [Tensor Flow](https://www.tensorflow.org/) without you needing to know either of these underlying frameworks. It provides implementations of several of the layer architectures, objective functions, and optimization algorithms you need for building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Dataset</font>\n",
    "\n",
    "My research is on story generation, so I've selected a dataset of stories as the text to be modeled by the RNN. They come from the [ROCStories](http://cs.rochester.edu/nlp/rocstories/) dataset, which consists of thousands of five-sentence stories about everyday life events. Here the model will observe all five sentences in each story. Then we'll use the trained model to generate the final sentence in a set of stories not observed during training. The full dataset is available at the above link and just requires filling out a form to get access. Here, I'll use a sample of 100 stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function #Python 2/3 compatibility for print statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll load the datasets using the [pandas library](https://pandas.pydata.org/), which is extremely useful for any task involving data storage and manipulation. This library puts a dataset into a readable table format, and makes it easy to retrieve specific columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sam always wanted to win. He never got the cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kate was at her garbage can on a dark night. A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jake's local movie theater had a horror movie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The man swept the room. The woman dropped some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was at an outdoor mall. But it was really ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>David is a large man. David does not eat healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rich was a musician. He made a few hit songs. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sam was a star athlete. He ran track at colleg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Carl wanted a snack to watch television. He go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I am a member of a clan in an online game. I w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Story\n",
       "0  Sam always wanted to win. He never got the cha...\n",
       "1  Kate was at her garbage can on a dark night. A...\n",
       "2  Jake's local movie theater had a horror movie ...\n",
       "3  The man swept the room. The woman dropped some...\n",
       "4  I was at an outdoor mall. But it was really ho...\n",
       "5  David is a large man. David does not eat healt...\n",
       "6  Rich was a musician. He made a few hit songs. ...\n",
       "7  Sam was a star athlete. He ran track at colleg...\n",
       "8  Carl wanted a snack to watch television. He go...\n",
       "9  I am a member of a clan in an online game. I w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load the training dataset'''\n",
    "\n",
    "import pandas\n",
    "\n",
    "train_stories = pandas.read_csv('dataset/example_train_stories.csv', encoding='utf-8')#[:100]\n",
    "\n",
    "train_stories[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Preparing the data</font>\n",
    "\n",
    "The model we'll create is a word-based language model, which means each input unit is a single word (some language models learn subword units like characters). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Tokenization</font>\n",
    "\n",
    "The first pre-processing step is to tokenize each of the reviews into (lowercased) individual words, since the RNN will encode the reviews word by word. For this I'll use [spacy](https://spacy.io/), which is a fast and extremely user-friendly library that performs various language processing tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story</th>\n",
       "      <th>Tokenized_Story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sam always wanted to win. He never got the cha...</td>\n",
       "      <td>[sam, always, wanted, to, win, ., he, never, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kate was at her garbage can on a dark night. A...</td>\n",
       "      <td>[kate, was, at, her, garbage, can, on, a, dark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jake's local movie theater had a horror movie ...</td>\n",
       "      <td>[jake, 's, local, movie, theater, had, a, horr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The man swept the room. The woman dropped some...</td>\n",
       "      <td>[the, man, swept, the, room, ., the, woman, dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was at an outdoor mall. But it was really ho...</td>\n",
       "      <td>[i, was, at, an, outdoor, mall, ., but, it, wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>David is a large man. David does not eat healt...</td>\n",
       "      <td>[david, is, a, large, man, ., david, does, not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Rich was a musician. He made a few hit songs. ...</td>\n",
       "      <td>[rich, was, a, musician, ., he, made, a, few, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sam was a star athlete. He ran track at colleg...</td>\n",
       "      <td>[sam, was, a, star, athlete, ., he, ran, track...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Carl wanted a snack to watch television. He go...</td>\n",
       "      <td>[carl, wanted, a, snack, to, watch, television...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I am a member of a clan in an online game. I w...</td>\n",
       "      <td>[i, am, a, member, of, a, clan, in, an, online...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Story  \\\n",
       "0  Sam always wanted to win. He never got the cha...   \n",
       "1  Kate was at her garbage can on a dark night. A...   \n",
       "2  Jake's local movie theater had a horror movie ...   \n",
       "3  The man swept the room. The woman dropped some...   \n",
       "4  I was at an outdoor mall. But it was really ho...   \n",
       "5  David is a large man. David does not eat healt...   \n",
       "6  Rich was a musician. He made a few hit songs. ...   \n",
       "7  Sam was a star athlete. He ran track at colleg...   \n",
       "8  Carl wanted a snack to watch television. He go...   \n",
       "9  I am a member of a clan in an online game. I w...   \n",
       "\n",
       "                                     Tokenized_Story  \n",
       "0  [sam, always, wanted, to, win, ., he, never, g...  \n",
       "1  [kate, was, at, her, garbage, can, on, a, dark...  \n",
       "2  [jake, 's, local, movie, theater, had, a, horr...  \n",
       "3  [the, man, swept, the, room, ., the, woman, dr...  \n",
       "4  [i, was, at, an, outdoor, mall, ., but, it, wa...  \n",
       "5  [david, is, a, large, man, ., david, does, not...  \n",
       "6  [rich, was, a, musician, ., he, made, a, few, ...  \n",
       "7  [sam, was, a, star, athlete, ., he, ran, track...  \n",
       "8  [carl, wanted, a, snack, to, watch, television...  \n",
       "9  [i, am, a, member, of, a, clan, in, an, online...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Split texts into lists of words (tokens)'''\n",
    "\n",
    "import spacy\n",
    "\n",
    "encoder = spacy.load('en')\n",
    "\n",
    "def text_to_tokens(text_seqs):\n",
    "    token_seqs = [[word.lower_ for word in encoder(text_seq)] for text_seq in text_seqs]\n",
    "    return token_seqs\n",
    "\n",
    "train_stories['Tokenized_Story'] = text_to_tokens(train_stories['Story'])\n",
    "    \n",
    "train_stories[['Story','Tokenized_Story']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Lexicon</font>\n",
    "\n",
    "Then we need to assemble a lexicon (aka vocabulary) of words that the model needs to know. Each tokenized word in the stories is added to the lexicon, and then each word is mapped to a numerical index that can be read by the model. Since large datasets may contain a huge number of unique words, it's common to filter all words occurring less than a certain number of times, and replace them with some generic &lt;UNK&gt; token. The min_freq parameter in the function below defines this threshold. In the example code, the min_freq parameter is set to 1, so the lexicon will contain all unique words in the training set. When assigning the indices, the number 1 will represent unknown words. The number 0 will represent \"empty\" word slots, which is explained below. Therefore \"real\" words will have indices of 2 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'all', 2), (u'go', 3), (u'kate', 4), (u'to', 5), (u'finally', 6), (u'sent', 7), (u'customer', 65), (u'woman', 9), (u'returned', 10), (u'outside', 11), (u'very', 12), (u'horror', 13), (u'detention', 14), (u'vegas', 15), (u'fall', 16), (u'stabbed', 17), (u'school', 18), (u'did', 19), (u'jamie', 20), (u'large', 21)]\n",
      "412 words in lexicon\n"
     ]
    }
   ],
   "source": [
    "'''Count tokens (words) in texts and add them to the lexicon'''\n",
    "\n",
    "import pickle\n",
    "\n",
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    # First, count how often each word appears in the text.\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    # Indices start at 2. 0 is reserved for padding, and 1 for unknown words.\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    print(list(lexicon.items())[:20])\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "lexicon = make_lexicon(token_seqs=train_stories['Tokenized_Story'], min_freq=1)\n",
    "print(\"{} words in lexicon\".format(len(lexicon)))\n",
    "\n",
    "with open('example_model/lexicon.pkl', 'wb') as f: # Save the lexicon by pickling it\n",
    "    pickle.dump(lexicon, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the model will output tags as indices, we'll obviously need to map each tag number back to its corresponding string representation in order to later interpret the output. We'll reverse the tags lexicon to create a lookup table to get each tag from its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, ''), (1, u'<UNK>'), (2, u'all'), (3, u'go'), (4, u'kate'), (5, u'to'), (6, u'finally'), (7, u'sent'), (8, u'me'), (9, u'woman'), (10, u'returned'), (11, u'outside'), (12, u'very'), (13, u'horror'), (14, u'detention'), (15, u'vegas'), (16, u'fall'), (17, u'stabbed'), (18, u'school'), (19, u'did')]\n"
     ]
    }
   ],
   "source": [
    "'''Make a dictionary where the string representation of a lexicon item can be retrieved from its numerical index'''\n",
    "\n",
    "def get_lexicon_lookup(lexicon):\n",
    "    lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "    lexicon_lookup[0] = \"\" #map 0 padding to empty string\n",
    "    print(list(lexicon_lookup.items())[:20])\n",
    "    return lexicon_lookup\n",
    "\n",
    "lexicon_lookup = get_lexicon_lookup(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>From strings to numbers</font>\n",
    "\n",
    "Once the lexicon is built, we can use it to transform each story from string tokens into a list of numerical indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Story</th>\n",
       "      <th>Story_Idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[sam, always, wanted, to, win, ., he, never, g...</td>\n",
       "      <td>[188, 121, 143, 5, 46, 359, 375, 36, 138, 303,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[kate, was, at, her, garbage, can, on, a, dark...</td>\n",
       "      <td>[4, 366, 389, 360, 354, 74, 346, 91, 320, 326,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[jake, 's, local, movie, theater, had, a, horr...</td>\n",
       "      <td>[315, 56, 219, 248, 297, 311, 91, 13, 248, 326...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[the, man, swept, the, room, ., the, woman, dr...</td>\n",
       "      <td>[303, 122, 237, 303, 66, 359, 303, 9, 80, 331,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, was, at, an, outdoor, mall, ., but, it, wa...</td>\n",
       "      <td>[298, 366, 389, 387, 304, 73, 359, 370, 283, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[david, is, a, large, man, ., david, does, not...</td>\n",
       "      <td>[150, 282, 91, 21, 122, 359, 150, 338, 118, 37...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[rich, was, a, musician, ., he, made, a, few, ...</td>\n",
       "      <td>[217, 366, 91, 60, 359, 375, 376, 91, 54, 221,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[sam, was, a, star, athlete, ., he, ran, track...</td>\n",
       "      <td>[188, 366, 91, 399, 71, 359, 375, 189, 337, 38...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[carl, wanted, a, snack, to, watch, television...</td>\n",
       "      <td>[142, 143, 91, 214, 5, 223, 178, 359, 375, 138...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[i, am, a, member, of, a, clan, in, an, online...</td>\n",
       "      <td>[298, 386, 91, 290, 350, 91, 324, 286, 387, 19...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Tokenized_Story  \\\n",
       "0  [sam, always, wanted, to, win, ., he, never, g...   \n",
       "1  [kate, was, at, her, garbage, can, on, a, dark...   \n",
       "2  [jake, 's, local, movie, theater, had, a, horr...   \n",
       "3  [the, man, swept, the, room, ., the, woman, dr...   \n",
       "4  [i, was, at, an, outdoor, mall, ., but, it, wa...   \n",
       "5  [david, is, a, large, man, ., david, does, not...   \n",
       "6  [rich, was, a, musician, ., he, made, a, few, ...   \n",
       "7  [sam, was, a, star, athlete, ., he, ran, track...   \n",
       "8  [carl, wanted, a, snack, to, watch, television...   \n",
       "9  [i, am, a, member, of, a, clan, in, an, online...   \n",
       "\n",
       "                                          Story_Idxs  \n",
       "0  [188, 121, 143, 5, 46, 359, 375, 36, 138, 303,...  \n",
       "1  [4, 366, 389, 360, 354, 74, 346, 91, 320, 326,...  \n",
       "2  [315, 56, 219, 248, 297, 311, 91, 13, 248, 326...  \n",
       "3  [303, 122, 237, 303, 66, 359, 303, 9, 80, 331,...  \n",
       "4  [298, 366, 389, 387, 304, 73, 359, 370, 283, 3...  \n",
       "5  [150, 282, 91, 21, 122, 359, 150, 338, 118, 37...  \n",
       "6  [217, 366, 91, 60, 359, 375, 376, 91, 54, 221,...  \n",
       "7  [188, 366, 91, 399, 71, 359, 375, 189, 337, 38...  \n",
       "8  [142, 143, 91, 214, 5, 223, 178, 359, 375, 138...  \n",
       "9  [298, 386, 91, 290, 350, 91, 324, 286, 387, 19...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Convert each text from a list of tokens to a list of numbers (indices)'''\n",
    "\n",
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "train_stories['Story_Idxs'] = tokens_to_idxs(token_seqs=train_stories['Tokenized_Story'],\n",
    "                                             lexicon=lexicon)\n",
    "                                   \n",
    "train_stories[['Tokenized_Story', 'Story_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Creating a matrix</font>\n",
    "\n",
    "Finally, we need to put all the training stories into a single matrix, where each row is a story and each column is a word index in that story. This enables the model to process the stories in batches as opposed to one at a time, which significantly speeds up training. However, each story has a different number of words. So we create a padded matrix equal to the length on the longest story in the training set. For all stories with fewer words, we prepend the row with zeros representing an empty word position. Then we can actually tell Keras to ignore these zeros during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ..., 108 140 359]\n",
      " [  0   0   0 ...,  44 360 359]\n",
      " [  0   0   0 ..., 311 255 359]\n",
      " ..., \n",
      " [  0   0   0 ..., 303 352 359]\n",
      " [  0   0   0 ..., 277 163 359]\n",
      " [  0   0   0 ..., 360 325 359]]\n",
      "SHAPE: (24, 62)\n"
     ]
    }
   ],
   "source": [
    "'''create a padded matrix of stories'''\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in train_stories['Story_Idxs']]) # Get length of longest sequence\n",
    "\n",
    "train_padded_idxs = pad_sequences(train_stories['Story_Idxs'], maxlen=max_seq_len)\n",
    "print(train_padded_idxs) #same example story as above\n",
    "\n",
    "print(\"SHAPE:\", train_padded_idxs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Defining the input and output</font>\n",
    "\n",
    "In an RNN language model, the data is set up so that each word in the text is mapped to the word that follows it. In a given story, for each input word x[idx], the output label y[idx] is just x[idx+1]. In other words, the output sequences (y) matrix will be offset by one to the right. The example below displays this alignment with the string tokens for the first story in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input Word</th>\n",
       "      <th>Output Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>sam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sam</td>\n",
       "      <td>always</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>always</td>\n",
       "      <td>wanted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wanted</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>win</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>win</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>.</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>he</td>\n",
       "      <td>never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>never</td>\n",
       "      <td>got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>got</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>the</td>\n",
       "      <td>chance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>chance</td>\n",
       "      <td>until</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>until</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>he</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>was</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>in</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>his</td>\n",
       "      <td>twenties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>twenties</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>.</td>\n",
       "      <td>sam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sam</td>\n",
       "      <td>decided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>decided</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>to</td>\n",
       "      <td>enter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>enter</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>a</td>\n",
       "      <td>taco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>taco</td>\n",
       "      <td>eating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>eating</td>\n",
       "      <td>competition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>competition</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>.</td>\n",
       "      <td>surprisingly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>surprisingly</td>\n",
       "      <td>enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>enough</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>he</td>\n",
       "      <td>actually</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>actually</td>\n",
       "      <td>won</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>won</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>.</td>\n",
       "      <td>sam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>sam</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>was</td>\n",
       "      <td>excited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>excited</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>for</td>\n",
       "      <td>weeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>weeks</td>\n",
       "      <td>after</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>after</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Input Word   Output Word\n",
       "0              -           sam\n",
       "1            sam        always\n",
       "2         always        wanted\n",
       "3         wanted            to\n",
       "4             to           win\n",
       "5            win             .\n",
       "6              .            he\n",
       "7             he         never\n",
       "8          never           got\n",
       "9            got           the\n",
       "10           the        chance\n",
       "11        chance         until\n",
       "12         until            he\n",
       "13            he           was\n",
       "14           was            in\n",
       "15            in           his\n",
       "16           his      twenties\n",
       "17      twenties             .\n",
       "18             .           sam\n",
       "19           sam       decided\n",
       "20       decided            to\n",
       "21            to         enter\n",
       "22         enter             a\n",
       "23             a          taco\n",
       "24          taco        eating\n",
       "25        eating   competition\n",
       "26   competition             .\n",
       "27             .  surprisingly\n",
       "28  surprisingly        enough\n",
       "29        enough            he\n",
       "30            he      actually\n",
       "31      actually           won\n",
       "32           won             .\n",
       "33             .           sam\n",
       "34           sam           was\n",
       "35           was       excited\n",
       "36       excited           for\n",
       "37           for         weeks\n",
       "38         weeks         after\n",
       "39         after             ."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame(list(zip([\"-\"] + train_stories['Tokenized_Story'].loc[0],\n",
    "                          train_stories['Tokenized_Story'].loc[0])),\n",
    "                 columns=['Input Word', 'Output Word'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the padded matrices the same length, the input word matrix will also both be offset by one in the opposite direction. So the length of both the input and output matrices will be both reduced by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Input Words  Output Words\n",
      "0             0             0\n",
      "1             0             0\n",
      "2             0             0\n",
      "3             0             0\n",
      "4             0             0\n",
      "5             0             0\n",
      "6             0             0\n",
      "7             0             0\n",
      "8             0             0\n",
      "9             0             0\n",
      "10            0             0\n",
      "11            0             0\n",
      "12            0             0\n",
      "13            0             0\n",
      "14            0             0\n",
      "15            0             0\n",
      "16            0             0\n",
      "17            0             0\n",
      "18            0             0\n",
      "19            0             0\n",
      "20            0             0\n",
      "21            0           188\n",
      "22          188           121\n",
      "23          121           143\n",
      "24          143             5\n",
      "25            5            46\n",
      "26           46           359\n",
      "27          359           375\n",
      "28          375            36\n",
      "29           36           138\n",
      "..          ...           ...\n",
      "31          303           402\n",
      "32          402            57\n",
      "33           57           375\n",
      "34          375           366\n",
      "35          366           286\n",
      "36          286           220\n",
      "37          220           228\n",
      "38          228           359\n",
      "39          359           188\n",
      "40          188           242\n",
      "41          242             5\n",
      "42            5            93\n",
      "43           93            91\n",
      "44           91           158\n",
      "45          158           341\n",
      "46          341           333\n",
      "47          333           359\n",
      "48          359            61\n",
      "49           61           369\n",
      "50          369           375\n",
      "51          375           105\n",
      "52          105           251\n",
      "53          251           359\n",
      "54          359           188\n",
      "55          188           366\n",
      "56          366           314\n",
      "57          314           336\n",
      "58          336           108\n",
      "59          108           140\n",
      "60          140           359\n",
      "\n",
      "[61 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pandas.DataFrame(list(zip(train_padded_idxs[0,:-1], train_padded_idxs[0, 1:])),\n",
    "                columns=['Input Words', 'Output Words']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='#6629b2'>Building the model</font>\n",
    "\n",
    "To assemble the model, we'll use Keras' [Functional API](https://keras.io/getting-started/functional-api-guide/), which is one of two ways to use Keras to assemble models (the alternative is the [Sequential API](https://keras.io/getting-started/sequential-model-guide/), which is a bit simpler but has more constraints). A model consists of a series of layers. As shown in the code below, we initialize instances for each layer. Each layer can be called with another layer as input, e.g. Embedding()(input_layer). A model instance is initialized with the Model() object, which defines the initial input and final output layers for that model. Before the model can be trained, the compile() function must be called with the loss function and optimization algorithm specified (see below).\n",
    "\n",
    "###  <font color='#6629b2'>Layers</font>\n",
    "\n",
    "We'll build an RNN with five layers:\n",
    "\n",
    "**1. Input**: The input layer takes in the matrix of word indices.\n",
    "\n",
    "**2. Embedding**: An [embedding input layer](https://keras.io/layers/embeddings/) that converts word indices into distributed vector representations (embeddings). The mask_zero=True parameter indicates that values of 0 in the matrix (the padding) will be ignored by the model.\n",
    "\n",
    "**3. GRU**: A [recurrent (GRU) hidden layer](https://keras.io/layers/recurrent/), the central component of the model. As it observes each word in the story, it integrates the word embedding representation with what it's observed so far to compute a representation (hidden state) of the review at that timepoint. There are a few architectures for this layer - I use the GRU variation, Keras also provides LSTM or just the simple vanilla recurrent layer (see the materials at the bottom for an explanation of the difference). By setting return_sequences=True for this layer, it will output the hidden states for every timepoint in the model, i.e. for every word in the story.\n",
    "\n",
    "**4. GRU**: A second recurrent layer that takes the first as input and operates the same way, since adding more layers generally improves the model. Rather than returning the sequence of values for each word like the previous hidden layer, this layer will output just the last hidden state of the sequence (i.e. the hidden representation of the story after its last word is observed), since by default the return_sequences parameter is False.\n",
    "\n",
    "**5. (Time Distributed) Dense**: A [dense output layer](https://keras.io/layers/core/#dense) that outputs a probability for each word in the lexicon, where each probability indicates the chance of that word being the next word in the sequence. The 'softmax' activation is what transforms the values of this layer into scores from 0 to 1 that can be treated as probabilities. The Dense layer produces the probability scores for one particular timepoint (word). By wrapping this in a TimeDistributed() layer, the model outputs a probability distribution for every timepoint in the sequence.\n",
    "\n",
    "The term \"layer\" is just an abstraction, when really all these layers are just matrices. The \"weights\" that connect the layers are also matrices. The process of training a neural network is a series of matrix multiplications. The weight matrices are the values that are adjusted during training in order for the model to learn to predict the next word.\n",
    "\n",
    "###  <font color='#6629b2'>Parameters</font>\n",
    "\n",
    "Our function for creating the model takes the following parameters:\n",
    "\n",
    "**seq_input_len:** the length of the input and output matrices. This is equal to the length of the longest story in the training data. \n",
    "\n",
    "**n_input_nodes**: the number of unique words in the lexicon, plus one to account for the padding represented by 0 values. This indicates the number of rows in the embedding layer, where each row corresponds to a word. It is also the dimensionality of the probability vectors given as the model output.\n",
    "\n",
    "**n_embedding_nodes**: the number of dimensions (units) in the embedding layer, which can be freely defined. Here, it is set to 300.\n",
    "\n",
    "**n_hidden_nodes**: the number of dimensions in the hidden layers. Like the embedding layer, this can be freely chosen. Here, it is set to 500.\n",
    "\n",
    "**stateful**: By default, the GRU hidden layer will reset its state (i.e. its values will be 0s) each time a new set of sequences is read into the model.  However, when stateful=True is given, this parameter indicates that the GRU hidden layer should \"remember\" its state until it is explicitly told to forget it. In other words, the values in this layer will be carried over between separate calls to the training function. This is useful when processing long sequences, so that the model can iterate through chunks of the sequences rather than loading the entire matrix at the same time, which is memory-intensive. I'll show below how this setting is also useful when the model is used for word prediction after training. During training, the model will observe all words in a story at once, so stateful will be set to False. At prediction time, it will be set to True.\n",
    "\n",
    "**batch_size**: It is not always necessary to specify the batch size when setting up a Keras model. The fit() function will apply batch processing by default and the batch size can be given as a parameter. However, when a model is stateful, the batch size does need to be specified in the Input() layers. Here, for training, batch_size=None, so Keras will use its default batch size (which is 32). During prediction, the batch size will be set to 1.\n",
    "\n",
    "### <font color='#6629b2'>Procedure</font>\n",
    "\n",
    "The output of the model is a sequence of vectors, each with the same number of dimensions as the number of unique words (n_input_nodes). Each vector contains the predicted probability of each possible word appearing in that position in the sequence. Like all neural networks, RNNs learn by updating the parameters (weights) to optimize an objective (loss) function applied to the output. For this model, the objective is to minimize the cross entropy (named as \"sparse_categorical_crossentropy\" in the code) between the predicted word probabilities and the probabilities observed from the words that appear in the training data, resulting in probabilities that more accurately predict when a particular word will appear. This is the general procedure used for all multi-label classification tasks. Updates to the weights of the model are performed using an optimization algorithm, such as Adam used here. The details of this process are extensive; see the resources at the bottom of the notebook if you want a deeper understanding. One huge benefit of Keras is that it implements many of these details for you. Not only does it already have implementations of the types of layer architectures, it also has many of the [loss functions](https://keras.io/losses/) and [optimization methods](https://keras.io/optimizers/) you need for training various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Create the model'''\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "def create_model(seq_input_len, n_input_nodes, n_embedding_nodes, \n",
    "                 n_hidden_nodes, stateful=False, batch_size=None):\n",
    "    \n",
    "    # Layer 1\n",
    "    input_layer = Input(batch_shape=(batch_size, seq_input_len), name='input_layer')\n",
    "\n",
    "    # Layer 2\n",
    "    embedding_layer = Embedding(input_dim=n_input_nodes, \n",
    "                                output_dim=n_embedding_nodes, \n",
    "                                mask_zero=True, name='embedding_layer')(input_layer) #mask_zero=True will ignore padding\n",
    "    # Output shape = (batch_size, seq_input_len, n_embedding_nodes)\n",
    "\n",
    "    #Layer 3\n",
    "    gru_layer1 = GRU(n_hidden_nodes,\n",
    "                     return_sequences=True, #return hidden state for each word, not just last one\n",
    "                     stateful=stateful, name='hidden_layer1')(embedding_layer)\n",
    "    # Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "\n",
    "    #Layer 4\n",
    "    gru_layer2 = GRU(n_hidden_nodes,\n",
    "                     return_sequences=True,\n",
    "                     stateful=stateful, name='hidden_layer2')(gru_layer1)\n",
    "    # Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "\n",
    "    #Layer 5\n",
    "    output_layer = TimeDistributed(Dense(n_input_nodes,\n",
    "                                         activation=\"softmax\"), name='output_layer')(gru_layer2)\n",
    "    # Output shape = (batch_size, seq_input_len, n_input_nodes)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    #Specify loss function and optimization algorithm, compile model\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = create_model(seq_input_len=train_padded_idxs.shape[-1] - 1, #substract 1 from matrix length because of offset \n",
    "                     n_input_nodes = len(lexicon) + 1, # Add 1 to account for 0 padding\n",
    "                     n_embedding_nodes = 300,\n",
    "                     n_hidden_nodes = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Training</font>\n",
    "\n",
    "Now we're ready to train the model. We'll call the fit() function to train the model for 10 iterations through the dataset (epochs), with a batch size of 20 stories. Keras reports the cross-entropy loss after each epoch - if the model is learning correctly, it should progressively decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '14660' (I am process '15037')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 [==============================] - 1s - loss: 6.0224\n",
      "Epoch 2/10\n",
      "24/24 [==============================] - 1s - loss: 5.9999\n",
      "Epoch 3/10\n",
      "24/24 [==============================] - 1s - loss: 5.9690\n",
      "Epoch 4/10\n",
      "24/24 [==============================] - 1s - loss: 5.9092\n",
      "Epoch 5/10\n",
      "24/24 [==============================] - 1s - loss: 5.7599\n",
      "Epoch 6/10\n",
      "24/24 [==============================] - 1s - loss: 5.4308\n",
      "Epoch 7/10\n",
      "24/24 [==============================] - 1s - loss: 5.6063\n",
      "Epoch 8/10\n",
      "24/24 [==============================] - 1s - loss: 5.3833\n",
      "Epoch 9/10\n",
      "24/24 [==============================] - 1s - loss: 5.4994\n",
      "Epoch 10/10\n",
      "24/24 [==============================] - 1s - loss: 5.5252\n"
     ]
    }
   ],
   "source": [
    "'''Train the model'''\n",
    "\n",
    "# output matrix (y) has extra 3rd dimension added because sparse cross-entropy function requires one label per row\n",
    "model.fit(x=train_padded_idxs[:,1:], y=train_padded_idxs[:, 1:, None], epochs=10, batch_size=20)\n",
    "model.save_weights('example_model/model_weights.h5') #Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Prediction Tasks</font>\n",
    "\n",
    "Now that the model is trained, we can use it for prediction. I'll show two prediction tasks: computing a probability score for a story, and generating a new ending for a story. To demonstrate both of these, I'll load a saved model previously trained on all the 96,000 stories in the training set. As opposed to training where we processed multiple stories at the same time, it will be more straightforward to demonstrate prediction on a single story at a time, especially since prediction is fast relative to training. In Keras, you can duplicate a model by loading the parameters from a saved model into a new model. Here, this new model will have a batch size of 1. It will also process a story one word at a time (seq_input_len=1), using the stateful=True parameter to remember the story that has occurred up to that word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Load test set and apply same processing used for training stories'''\n",
    "\n",
    "test_stories = pandas.read_csv('dataset/example_test_stories.csv', encoding='utf-8')\n",
    "test_stories['Tokenized_Story'] = text_to_tokens(test_stories['Story'])\n",
    "test_stories['Story_Idxs'] = tokens_to_idxs(token_seqs=test_stories['Tokenized_Story'],\n",
    "                                            lexicon=lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Create a new test model, setting batch_size = 1, seq_input_len = 1, and stateful = True'''\n",
    "\n",
    "# Load lexicon from the saved model \n",
    "with open('example_model/lexicon.pkl', 'rb') as f:\n",
    "    lexicon = pickle.load(f)\n",
    "\n",
    "predictor_model = create_model(seq_input_len=1,\n",
    "                               n_input_nodes=len(lexicon) + 1,\n",
    "                               n_embedding_nodes = 300,\n",
    "                               n_hidden_nodes = 500,\n",
    "                               stateful=True, \n",
    "                               batch_size = 1)\n",
    "\n",
    "predictor_model.load_weights('pretrained_model/model_weights.h5') #Load weights from saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Computing story probabilities</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model outputs a probability distribution for each word in the story, indicating the probability of each possible next word in the story, we can use these values to get a single probability score for the story. To do this, we iterate through each word in a story, call the predict() function to get the full list of probabilites for the next word, and then extract the probability predicted for the actual next word in the story. We can average these probabilities across all words in the story to get a single value. The stateful=True parameter is what enables the model to remember the previous words in the story when predicting the probability of the next word. Becuase of this, the reset_states() function must be called at the end of reading the story in order to clear its memory for the next story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STORY: Jervis decides he wants to vote in the upcoming election. He votes for the Mayor. Unfortunately his candidate does not win. Even though his candidate didn't win, Jervis is happy he voted. He decides he will vote in the next election too.\n",
      "PROBABILITY: 0.00343944\n"
     ]
    }
   ],
   "source": [
    "'''Compute the probability of a sequence according to the language model'''\n",
    "\n",
    "import numpy\n",
    "\n",
    "def get_probability(idx_seq):\n",
    "    idx_seq = [0] + idx_seq #Prepend 0 so first call to predict() computes prob of first word from zero padding\n",
    "    probs = []\n",
    "    for word, next_word in zip(idx_seq[:-1], idx_seq[1:]):\n",
    "       # Word is an integer, but the model expects an input array\n",
    "       # with the shape (batch_size, seq_input_len), so prepend two dimensions\n",
    "        p_next_word = predictor_model.predict(numpy.array(word)[None,None])[0,0] #Output shape= (lexicon_size + 1,)\n",
    "        #Select predicted prob of the next word, which appears in the corresponding idx position of the probability vector\n",
    "        p_next_word = p_next_word[next_word]\n",
    "        probs.append(p_next_word)\n",
    "    predictor_model.reset_states()\n",
    "    return numpy.mean(probs) #return average probability of words in sequence\n",
    "\n",
    "print(\"STORY:\", test_stories['Story'].loc[0])\n",
    "print(\"PROBABILITY:\", get_probability(test_stories['Story_Idxs'].loc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we randomly shuffle all the words around in a story, for instance, the model should assign a much lower probability to the story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHUFFLED STORY: election not he . too mayor though will election even is the wants , he his jervis . he jervis win win candidate to decides decides happy in in upcoming n't he vote voted did he next . vote the his . . votes unfortunately for candidate does the\n",
      "PROBABILITY: 0.0035048\n"
     ]
    }
   ],
   "source": [
    "shuffled_word_positions = numpy.random.permutation(len(test_stories['Tokenized_Story'].loc[0]))\n",
    "shuffled_token_story = [test_stories['Tokenized_Story'].loc[0][position] for position in shuffled_word_positions]\n",
    "shuffled_idx_story = [test_stories['Story_Idxs'].loc[0][position] for position in shuffled_word_positions]\n",
    "print(\"SHUFFLED STORY:\", \" \".join(shuffled_token_story))\n",
    "print(\"PROBABILITY:\", get_probability(shuffled_idx_story))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Generating sentences</font>\n",
    "\n",
    "The language model can also be used to generate new text. Here, I'll give the same predictor model the first four sentences of a story in the test set and have it generate the fifth sentence. To do this, we \"load\" the first four sentences into the model. This can be done using predict() function. Because the model is stateful, predict() saves the representation of the story internally even though we don't need the output of this function when just reading the story. Once the final word in the fourth sentence has been read, then we can start using the resulting probability distribution to predict the first word in the fifth sentence. We can call numpy.random.choice() to randomly sample a word according to its probability. Now we again call predict() with this new word as input, which returns a probability distribution for the second word. Again, we sample from this distribution, append the newly sampled word to the previously generated word, and call predict() with this new word as input. We continue doing this until a word that ends with an end-of-sentence puncutation mark (\".\", \"!\", \"?\") has been selected. Just as before, reset_states() is called after the whole sentence has been generated. Then we can decode the generated ending into a string using the lexicon lookup dictionary. You can see that the generated endings are generally not as coherent and well-formed as the human-authored endings, but they do capture some components of the story and they are often more entertaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIAL STORY: jervis decides he wants to vote in the upcoming election . he votes for the mayor . unfortunately his candidate does not win . even though his candidate did n't win , jervis is happy he voted .\n",
      "GIVEN ENDING: he decides he will vote in the next election too .\n",
      "GENERATED ENDING: knew . \n",
      "\n",
      "INITIAL STORY: sam always wanted to win . he never got the chance until he was in his twenties . sam decided to enter a taco eating competition . surprisingly enough he actually won .\n",
      "GIVEN ENDING: sam was excited for weeks after .\n",
      "GENERATED ENDING: think wife chip big sell club hit track woman price he girl up the heavy dark pair vegas fans store tree glad stuff store flavor nobody developed down my returned stabbed card no insist too hospital mexican process some loved cancer rage tried heel anyone approaches win up theater months other walked eventually plans decided detention david sell steak falls weird lots member janice could brat looked train before turned heel loved was exercising thrilled his behind clan her some high he stared behind knees with heel until soccer rag men jane star ones it to cancer is thrilled broken online and went think slips men with the marrying only just rug dug line surprisingly come armpits students healthy lives me am watch large him kate member steak friends chip before coming a on once having lives a green my old different her rug detention race around evening . \n",
      "\n",
      "INITIAL STORY: kate was at her garbage can on a dark night . and a raccoon was standing near the can . it started to come towards her . kate turned and ran to the house hoping it was n't behind her .\n",
      "GIVEN ENDING: once inside was relieved to see it had n't followed her .\n",
      "GENERATED ENDING: pointed waited tons hit knew hit empty up before night weeks dug wash fight girl doing life followed legs would songs step sam chest one developed glad steak garbage after wife working started dealer , looked go it night goes break her dug watch another movie jane fans old convinced realize fell big second around actually ran glasses marrying woman together always actually able unfortunately bit already plans moves wife not stabbed healthy public dropped fighting craig head jamie girlfriend doors until healthy moves an finally i doing falls woman ruined sure able everyone dug chips eating brought bag head bad but grass feel large movie tree principal she feeling jamie room relieved told calls recreate it would for mad theater very too until store down chest had rich at stood he house night inside trouble food could behind girlfriend eat once were chest sure waited go tried no were member excited found finally told an sell for david game throwing jane fight turned nightmares away always fighting broke stepped diet highway them followed different owner ended man shelves developed in flavor approaches school towards woman mall second but watched not she  did weeks my am watched orange armpits new decided everyone soccer in clothing lot perfectly inside tastes cost so feel to surprisingly would to life owner athlete laughed afterwards cancer watched picked friend an can vegas out his  new taco he outside , . \n",
      "\n",
      "INITIAL STORY: jake 's local movie theater had a horror movie night . he loved horror movies . unfortunately his girlfriend did not . he finally convinced her to go see one .\n",
      "GIVEN ENDING: she was scared and had nightmares .\n",
      "GENERATED ENDING: stabbed squishy recipe school new rest it break chips starting jamie i turned traffic anyone break rag sure near store taped getting loved college hated go race is when inside spat owner diet laughed was through maia sold sale just old 2 pain dealer approaches ended turned around game through other taco insist told empty turned through second get maia horror home funeral spat i towards marrying back soccer who night glad clan excited because game about until local medications tree online coming vegas eating athlete road second walked right second had year sat about is gets am come lot mexican of garbage family steak was big drills week i he attack mall pair until standing made went chips soccer medications another member fight family for television for unfortunately cost . \n",
      "\n",
      "INITIAL STORY: the man swept the room . the woman dropped some food . the man got mad . the woman swept the room the next time .\n",
      "GIVEN ENDING: the man did n't care when she dropped food then .\n",
      "GENERATED ENDING: evening had television train american his them food train twenties glasses fit day wishing mad having when much can it in followed 2 mexican up convinced spat woman rich my from of relieved some down of mall songs just broke while tried bought see due about night twenties am 's his him rest nobody alone won break it not relieved customer sign said eventually hour wash weird down all mother steak she highway enter towards wash starting school large movie that does near convinced , heavy behind wiped television but once road large never empty as big dropped her told some night college chip she trouble wash large david was bad it cancer looked college towards their leave local outside very getting brought could enter could before rest liked eventually snack into enough jamie dropped sell never chips detention raccoon wiped his member vegas medications there started trash watched line rug he spat rag bad weird eventually a flavor customer orange hit up college from go until empty only rest stuff behind grass chip excited public did online much another knew earlier the he wash can once tried but heavy a stuff public falls wants so of mother me to eating theater home back chest as then finally resist sam due excited local room ones big clothing before out close store race brat perfectly 3 moves was thrilled garbage picked standing falls up songs get gets getting class . \n",
      "\n",
      "INITIAL STORY: i was at an outdoor mall . but it was really hot . i was getting very sweaty . so i decided to wash my armpits at an empty public bathroom .\n",
      "GIVEN ENDING: i was glad nobody walked in .\n",
      "GENERATED ENDING: swept cared calls stood n't rage sam uses plans new convinced go in week 1 detention starts other traffic mexican few before of room high actually bit not 's trash new watched throwing diagnosed high throwing love gets today different about hot second for other decided member is finally all different perfectly resist who slips rest maia pair liked star ate care recipe took that clean allowed glasses taped tastes nose sat process knew sell this diagnosed i wanted before clean feels man stepped hospital fight found had ate competitive  several orange scared think college star traffic maia pair detention up had think this cleaning lot doors took stood but soccer got had mall starts up back liked being got spent followed headed not television trouble bit spent enter fight my enter process army day knees while waited insist seat movie big right competitive glasses local diet chip skipped sam not pair raccoon throwing all down place decided process orange his started knees chip spent funeral fall medications all really an this las classroom get evening different before mall <UNK> entered behind lot ended customer family looked despite broken leave eating room watch closets swept everyone armpits started sat athlete mexican despite breaks that broke schedule despite rug perfectly clan owner nightmares squishy decided 2 customer to guy step rag . \n",
      "\n",
      "INITIAL STORY: david is a large man . david does not eat healthy things . one evening while eating his steak david starts feeling a pain . he feels a pain in his chest and falls to his knees .\n",
      "GIVEN ENDING: david is having a heart attack his wife calls for help .\n",
      "GENERATED ENDING: she because a hoping cared so raccoon recipe year able he an coming won a sell american traffic he plans pointed new mexican taco television girl while eat night gets remission him shop gone then david bit with wants due developed glad hospital fight surprisingly cost out musician hour head . \n",
      "\n",
      "INITIAL STORY: rich was a musician . he made a few hit songs . rich had a lot of fans who cared about him . he developed cancer .\n",
      "GIVEN ENDING: eventually the cancer took his life and rich was gone .\n",
      "GENERATED ENDING: rag up being not in towards television right watched back swept looked a carl nightmares alone healthy entered about time it being said stood hit doing movies rag trying trouble lives things line some fighting up threw everyone detention bathroom not twenties they scared excited her leave found chips at ones things schedule her alone next army starting bought uses large already cancer race cancer i bag the i that family spat shelves schedule about snack brought he am competition jane sign as classroom woman <UNK> hated up squishy night i very another always . \n",
      "\n",
      "INITIAL STORY: sam was a star athlete . he ran track at college . there was a big race coming up . everyone was sure he would win .\n",
      "GIVEN ENDING: sam got first place .\n",
      "GENERATED ENDING: night i up hospital her twenties started once hospital returned fit weird remission able jamie maia large already having knees ruined craig come ended breaks ones she fans bathroom chest process close clan dark cleaning woman n't . \n",
      "\n",
      "INITIAL STORY: carl wanted a snack to watch television . he got an opened bag of chips . he sat to watch . when he bit a chip it was squishy .\n",
      "GIVEN ENDING: carl spat it out and threw the chips away .\n",
      "GENERATED ENDING: an this another today home friends attack lot won line there detention vegas glad empty steak mother rest gets mall legs wrist nose wiped surprisingly fit cancer my 3 disappear and and ones friends mother high club wife stared drills movie mexican break lots breaks jane wife owner getting soccer jane empty help my competition soccer but waited my wishing fighting first around turned dug stabbed . \n",
      "\n",
      "INITIAL STORY: i am a member of a clan in an online game . i was asked to help train a new clan . i had never tried to train anyone before . after a few months the new clan was competitive .\n",
      "GIVEN ENDING: i did n't realize i knew that much about the game .\n",
      "GENERATED ENDING: visited maia man detention dad ruined head bit returned see tried everyone the woman 's slips race armpits vegas so inside songs there cared gets . \n",
      "\n",
      "INITIAL STORY: i went to the mall today . i had no plans , i just wanted to shop . i visited a new store and found a rug on sale . the rug was weird but it fit my tastes perfectly .\n",
      "GIVEN ENDING: despite the cost , i bought the rug and i am glad .\n",
      "GENERATED ENDING: away tree her recipe attack chips food chips class stuff that not but could anyone dad all today girlfriend sat getting . \n",
      "\n",
      "INITIAL STORY: sam was a car dealer . he tried to sell a car to a customer . sam tried to sell it at too high a price . the customer said they would think about it and go back .\n",
      "GIVEN ENDING: they never returned .\n",
      "GENERATED ENDING: nightmares that having actually returned las steak everyone club weeks knees stared watch friend ran a tried hoping n't really eventually decided sell gets rich large very . \n",
      "\n",
      "INITIAL STORY: earlier today i was stuck on the highway due to heavy traffic . there was a road rage fight and men were fighting on the highway . there were 3 men and it was 2 against 1 . the guy that was alone had a knife .\n",
      "GIVEN ENDING: he then stabbed the other 2 men and everyone ended up at the hospital .\n",
      "GENERATED ENDING: girlfriend pain orange on glad insist house to few stood all medications my bit sure things girl mall kate rug . \n",
      "\n",
      "INITIAL STORY: craig was diagnosed with cancer . he decided to fight it . he tried several different approaches and medications . eventually it went into remission .\n",
      "GIVEN ENDING: craig and his loved ones were thrilled .\n",
      "GENERATED ENDING: man earlier week shop because home . \n",
      "\n",
      "INITIAL STORY: i decided to clean out all the closets . i got a rag and a trash bag . i went into the closets and picked through the old clothing . after throwing out the old stuff , i started cleaning .\n",
      "GIVEN ENDING: i wiped down all the shelves and doors .\n",
      "GENERATED ENDING: class jane , competitive tried n't thrilled spent skipped train carl carl so wants several finally insist ended so there cost school clan ended shelves told second wrist dropped squishy cleaning down not class but so an eat ended american asked girlfriend trying seat dad grass american i always customer american walked was because   hour allowed feeling wiped songs followed men year one win high returned principal can can new in nobody lots there trouble sale one friend flavor step were mexican excited swept their the heel hot stuff first club disappear several entered trash coming snack television they ate about come online diet n't with closets new trying taped laughed headed tree ate diet convinced goes of enough road already knife trying member green surprisingly did another against one said old before allowed clothing chest feels <UNK> had new when . \n",
      "\n",
      "INITIAL STORY: kate and her friends were in line outside a club in las vegas . they waited close to an hour . they were finally able to go inside . and kate stepped on a step and broke her heel .\n",
      "GIVEN ENDING: her night was already ruined .\n",
      "GENERATED ENDING: bag . \n",
      "\n",
      "INITIAL STORY: i was trying to watch my diet . but my family brought home lots of mexican food . i could n't resist and dug right in . i ate tons of steak and rice .\n",
      "GIVEN ENDING: but i was glad that i did n't feel bad afterwards .\n",
      "GENERATED ENDING: lots bathroom sold was <UNK> feeling dropped not were relieved carl her friends disappear after jake this pain some dad come went taco another on jane much they wishing school old mall wants flavor bad wiped first never out bad twenties actually taco gets at wanted an swept squishy is just asked alone does dug the clan orange green ruined rage plans men a taped rage only tastes from come american school 3 day for school nose house closets had stuff watch a thrilled knees cared enter school wrist 1 american craig she train n't wants all got knees garbage able did pair had a after college convinced recreate wants fans wanted . \n",
      "\n",
      "INITIAL STORY: maia is an army brat , so she moves around a lot . today , she is starting a new school for the second time this year . she got her schedule and then went to her first class . as she headed to an empty seat , other students watched her .\n",
      "GIVEN ENDING: maia hated being the new girl at school .\n",
      "GENERATED ENDING: different get started he 's lot working bag stabbed assumes months months once actually about at against road life drills woman exercising time were me approaches track picked him earlier . \n",
      "\n",
      "INITIAL STORY: jane only had one pair of glasses . and she had broken them . her mother taped them up and sent her to school . when she entered the classroom everyone stared at her .\n",
      "GIVEN ENDING: they all pointed and laughed as she stood wishing she could disappear .\n",
      "GENERATED ENDING: hoping on hot the her night so to getting that new . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Use the model to generate new endings for stories'''\n",
    "\n",
    "def generate_ending(idx_seq):\n",
    "    \n",
    "    end_of_sent_tokens = [\".\", \"!\", \"?\"]\n",
    "    generated_ending = []\n",
    "    \n",
    "    # First just read initial story, no output needed\n",
    "    for word in idx_seq:\n",
    "        p_next_word = predictor_model.predict(numpy.array(word)[None,None])[0,0]\n",
    "        \n",
    "    # Now start predicting new words\n",
    "    while not generated_ending or lexicon_lookup[next_word] not in end_of_sent_tokens:\n",
    "        #Randomly sample a word from the current probability distribution\n",
    "        next_word = numpy.random.choice(a=p_next_word.shape[-1], p=p_next_word)\n",
    "        # Append sampled word to generated ending\n",
    "        generated_ending.append(next_word)\n",
    "        # Get probabilities for next word by inputing sampled word\n",
    "        p_next_word = predictor_model.predict(numpy.array(next_word)[None,None])[0,0]\n",
    "    \n",
    "    predictor_model.reset_states() #reset hidden state after generating ending\n",
    "    \n",
    "    return generated_ending\n",
    "\n",
    "for _, test_story in test_stories[:20].iterrows():\n",
    "    # Use spacy to segment the story into sentences, so we can seperate the ending sentence\n",
    "    # Find out where in the story the ending starts (number of words from end of story)\n",
    "    ending_story_idx = len(list(encoder(test_story['Story']).sents)[-1])\n",
    "    print(\"INITIAL STORY:\", \" \".join(test_story['Tokenized_Story'][:-ending_story_idx]))\n",
    "    print(\"GIVEN ENDING:\", \" \".join(test_story['Tokenized_Story'][-ending_story_idx:]))\n",
    "    \n",
    "    generated_ending = generate_ending(test_story['Story_Idxs'][:-ending_story_idx])\n",
    "    generated_ending = \" \".join([lexicon_lookup[word] if word in lexicon_lookup else \"\"\n",
    "                                 for word in generated_ending]) #decode from numbers back into words\n",
    "    print(\"GENERATED ENDING:\", generated_ending, \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Visualizing inner layers</font>\n",
    "\n",
    "To help visualize the data representation inside the model, we can look at the output of each layer individually. Keras' Functional API lets you derive a new model with the layers from an existing model, so you can define the output to be a layer below the output layer in the original model. Calling predict() on this new model will produce the output of that layer for a given input. Of course, glancing at the numbers by themselves doesn't provide any interpretation of what the model has learned (although there are opportunities to [interpret these values](https://www.civisanalytics.com/blog/interpreting-visualizing-neural-networks-text-processing/)), but seeing them verifies the model is just a series of transformations from one matrix to another. The model stores its layers in the list model.layers, and you can retrieve specific layer by its position index in the model. Below is an example of the word embedding output for the first word in the first story of the test set. You can do this same thing to view any layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD EMBEDDINGS OUTPUT SHAPE: (1, 1, 300)\n",
      "[[ -3.35724950e-02   2.86579095e-02  -5.21581993e-03  -1.10437647e-02\n",
      "   -1.08154863e-03   3.63551825e-03  -2.79936939e-03   4.44887839e-02\n",
      "    1.55534260e-02  -4.43191081e-03   2.88024060e-02   8.21934268e-03\n",
      "    6.38566911e-04  -4.00012359e-03  -3.92239615e-02   4.95298319e-02\n",
      "    1.20215118e-02   1.83434598e-02   1.94453448e-03  -1.62741393e-02\n",
      "   -4.32982706e-02   1.75365098e-02   1.05103850e-03   1.59489252e-02\n",
      "    2.65862681e-02   1.97608285e-02   9.03087854e-03  -2.26774905e-02\n",
      "    1.93058886e-02   3.74034829e-02   1.70300715e-02  -3.03554162e-02\n",
      "   -8.62551108e-03   4.13949825e-02   4.33015488e-02   4.48926575e-02\n",
      "   -3.98072079e-02  -1.50993802e-02   3.39943506e-02  -4.94189598e-02\n",
      "    1.18554942e-02   8.46051425e-03  -4.08301428e-02  -4.30781282e-02\n",
      "   -2.91937422e-02   4.38954197e-02   3.03712524e-02  -4.70339134e-03\n",
      "   -1.97810382e-02  -1.30449645e-02   3.56330611e-02  -4.25136350e-02\n",
      "    4.34095003e-02   2.25083269e-02   9.00292397e-03  -1.60483941e-02\n",
      "   -1.74494199e-02   1.03806444e-02   1.07565224e-02   3.65981273e-02\n",
      "   -2.43987534e-02   2.27716677e-02   4.99800183e-02  -3.16725038e-02\n",
      "   -2.14054640e-02  -3.81696969e-02   1.59446038e-02   1.34352595e-03\n",
      "    2.50987746e-02   2.20449157e-02  -3.35475802e-02  -4.91456613e-02\n",
      "   -3.92814614e-02   1.64122507e-03  -1.94576178e-02  -3.33440825e-02\n",
      "   -3.98625359e-02   4.60895933e-02  -1.78938918e-02  -3.79841514e-02\n",
      "    1.23194866e-02   1.93568580e-02  -2.48366147e-02   1.42913796e-02\n",
      "    3.25702131e-04   1.55390836e-02   3.43473814e-02  -4.59865183e-02\n",
      "   -2.11945418e-02  -1.23687759e-02   1.32613629e-03   4.62783761e-02\n",
      "   -2.99848616e-05   5.12366742e-03   2.48343013e-02  -3.15934643e-02\n",
      "   -1.75181776e-03  -1.29944831e-03  -3.05859838e-02  -4.50374596e-02\n",
      "   -7.04898685e-03   3.93644013e-02   4.09152694e-02  -3.32003906e-02\n",
      "    4.68457155e-02  -4.05834056e-02  -2.41475403e-02   1.23300068e-02\n",
      "    3.59160341e-02   3.48901711e-02   4.80178483e-02  -2.70617530e-02\n",
      "   -4.85495478e-02  -1.55752897e-03  -3.81155387e-02  -4.37041596e-02\n",
      "    6.21230155e-03   6.59127906e-03  -2.81877443e-03   2.49859281e-02\n",
      "    1.44016109e-02   2.96558775e-02   6.37795776e-03   2.61272378e-02\n",
      "    6.39259070e-03   2.77031101e-02  -4.85429056e-02  -1.19303726e-02\n",
      "    1.25323944e-02  -7.33391568e-03   4.55243923e-02   4.41113152e-02\n",
      "   -2.42685154e-03   3.19353528e-02   4.32033464e-03  -2.49414146e-02\n",
      "    4.34262492e-02   4.32229899e-02  -1.30711496e-03   4.69432734e-02\n",
      "    7.38841295e-03  -3.02949082e-02  -4.86123711e-02   5.03343344e-03\n",
      "   -1.72944590e-02  -3.12784612e-02  -2.09346768e-02   4.93635349e-02\n",
      "   -1.37339309e-02   2.81794257e-02  -8.67128000e-03   2.50481628e-02\n",
      "    3.14180143e-02   1.22590438e-02   3.05445604e-02   4.06526662e-02\n",
      "    2.66261809e-02  -3.24414708e-02  -2.57832482e-02   4.37153541e-02\n",
      "   -1.28560811e-02   3.09463702e-02   4.16407473e-02   3.83052565e-02\n",
      "    4.60642762e-02   3.31833102e-02   2.99398936e-02  -2.62539685e-02\n",
      "    2.86259465e-02  -2.81940214e-02   2.29930617e-02  -2.12523639e-02\n",
      "   -2.80447938e-02   1.98951624e-02   2.79210508e-04   1.05893314e-02\n",
      "    4.61611561e-02  -1.93013102e-02  -2.89172139e-02   5.28355688e-03\n",
      "   -2.60640960e-02  -4.74713743e-03  -7.60006160e-03   4.44347896e-02\n",
      "    4.17474471e-02  -4.84614410e-02   2.58830898e-02  -4.58479673e-03\n",
      "   -2.12210007e-02   2.47452967e-02  -4.08002250e-02  -4.49929684e-02\n",
      "   -3.68091911e-02   2.03291960e-02   8.11329484e-03   2.63292678e-02\n",
      "   -3.86219472e-04  -1.13929585e-02   4.32937481e-02   1.42980367e-04\n",
      "   -3.88987288e-02  -4.91228178e-02   3.10711749e-02   1.14848539e-02\n",
      "   -3.67193446e-02   1.97910145e-03   1.25891306e-02  -1.10152550e-02\n",
      "    3.13967355e-02   2.54453383e-02   4.04947065e-02   3.66348065e-02\n",
      "   -2.26147305e-02  -2.61786319e-02  -4.38402444e-02   2.91779377e-02\n",
      "   -3.20706554e-02   4.38799523e-02   4.76487838e-02  -3.51453274e-02\n",
      "   -4.62789126e-02   2.05198713e-02   2.00330578e-02   2.15525590e-02\n",
      "   -2.76997071e-02  -8.56417790e-03  -4.79089953e-02   2.18419768e-02\n",
      "   -4.82790917e-03   2.29137354e-02  -1.56006664e-02  -2.98430864e-02\n",
      "    4.40523587e-02   1.51396878e-02   2.11007781e-02   2.21014135e-02\n",
      "    3.91462557e-02  -1.78264976e-02  -1.92138497e-02  -3.04421410e-02\n",
      "   -4.44461815e-02  -2.14925557e-02  -4.14944440e-03   3.74782048e-02\n",
      "   -3.12962383e-02   2.75926329e-02  -4.97323982e-02  -8.07917863e-03\n",
      "    1.85048096e-02  -3.76885757e-03  -8.67710263e-03   1.01822205e-02\n",
      "   -3.09496801e-02  -4.70237583e-02   3.22941132e-02  -3.55752520e-02\n",
      "    4.48227860e-02  -6.21869788e-03  -2.56996136e-02  -4.84277941e-02\n",
      "   -3.47028673e-03  -1.09293461e-02   3.92336398e-04  -1.48962513e-02\n",
      "   -3.25921066e-02  -4.98866402e-02  -1.65883973e-02  -3.21330130e-02\n",
      "   -2.71270107e-02  -4.43196446e-02  -3.66594121e-02  -4.76090610e-02\n",
      "   -2.70212200e-02  -4.48514223e-02  -3.87491807e-02  -1.83165222e-02\n",
      "    1.65597759e-02  -1.16807297e-02  -1.41077675e-02  -2.63429862e-02\n",
      "    1.15475059e-03  -1.61143392e-02  -4.85805236e-02   1.47726052e-02\n",
      "    6.96301460e-04  -1.38693228e-02   8.86831805e-03   1.33779459e-02\n",
      "   -4.19833735e-02  -4.15803269e-02   4.01365757e-03   4.24762554e-02\n",
      "   -1.11820549e-02   4.37018909e-02  -2.88683102e-02   2.33842395e-02\n",
      "   -9.72837582e-03  -1.13540851e-02  -2.24379357e-02   2.55179740e-02]]\n"
     ]
    }
   ],
   "source": [
    "'''Show the output of the word embedding layer'''\n",
    "\n",
    "embedding_layer = Model(inputs=predictor_model.layers[0].input,\n",
    "                        outputs=predictor_model.layers[1].output)\n",
    "#Show output for first predicted tag in sequence (word input is first word, tag input is 0)\n",
    "embedding_output = embedding_layer.predict(numpy.array(test_stories['Story_Idxs'][0][0])[None,None])\n",
    "print(\"WORD EMBEDDINGS OUTPUT SHAPE:\", embedding_output.shape)\n",
    "print(embedding_output[0]) # Print embedding vectors for first word of first story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Conclusion</font>\n",
    "\n",
    "There are a good number of tutorials on RNN language models, particularly applied to text genertion. This one shows how to leverage Keras with batch training when the length of the sequences is variable. There are many ways this language model can be made to be more sophisticated. Here's a few interesting papers from the NLP community that innovate this basic model for different generation tasks:\n",
    "\n",
    "*Recipe generation:* [Globally Coherent Text Generation with Neural Checklist Models](https://homes.cs.washington.edu/~yejin/Papers/emnlp16_neuralchecklist.pdf). Chloé Kiddon, Luke Zettlemoyer, and Yejin Choi. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\n",
    "\n",
    "*Emotional text generation:* [Affect-LM: A Neural Language Model for Customizable Affective Text Generation](https://arxiv.org/pdf/1704.06851.pdf). Sayan Ghosh, Mathieu Chollet, Eugene Laksana, Louis-Philippe Morency, Stefan Scherer. Annual Meeting of the Association for Computational Linguistics (ACL), 2017.\n",
    "\n",
    "*Poetry generation:* [Generating Topical Poetry](https://www.isi.edu/natural-language/mt/emnlp16-poetry.pdf). Marjan Ghazvininejad, Xing Shi, Yejin Choi, and Kevin Knight. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\n",
    "\n",
    "*Dialogue generation:* [A Neural Network Approach to Context-Sensitive Generation of Conversational Responses](http://www-etud.iro.umontreal.ca/~sordonia/pdf/naacl15.pdf). Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie1, Jianfeng Gao, Bill Dolan. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>More resources</font>\n",
    "\n",
    "Yoav Goldberg's book [Neural Network Methods for Natural Language Processing](http://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037) is a thorough introduction to neural networks for NLP tasks in general\n",
    "\n",
    "If you'd like to learn more about what Keras is doing under the hood, the [Theano tutorials](http://deeplearning.net/tutorial/) are useful. There are two specifically on RNNs for NLP: [semantic parsing](http://deeplearning.net/tutorial/rnnslu.html#rnnslu) and [sentiment analysis](http://deeplearning.net/tutorial/lstm.html#lstm)\n",
    "\n",
    "TensorFlow also has an RNN language model [tutorial](https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html) using the Penn Treebank dataset\n",
    "\n",
    "Andrej Karpathy's blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) is very helpful for understanding the underlying details of the same language model I've demonstrated here. It also provides raw Python code with an implementation of the backpropagation algorithm.\n",
    "\n",
    "Chris Olah provides a good [explanation](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) of how LSTM RNNs work (this explanation also applies to the GRU model used here)\n",
    "\n",
    "Denny Britz's [tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) documents well both the technical details of RNNs and their implementation in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
