{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#6629b2'>Part-of-speech tagging with recurrent neural networks using Keras</font>\n",
    "### https://github.com/roemmele/keras-rnn-demo/pos-tagging\n",
    "by Melissa Roemmele, 10/23/17, roemmele @ ict.usc.edu\n",
    "\n",
    "## <font color='#6629b2'>Overview</font>\n",
    "\n",
    "I am going to show how to use the Keras library to build a recurrent neural network (RNN) model that labels part-of-speech (POS) tags for words in sentences. \n",
    "\n",
    "### <font color='#6629b2'>Part-of-speech (POS) tagging</font>\n",
    "\n",
    "A part-of-speech tag is the syntactic category associated with a particular word in a sentence, such as a noun, verb, preposition, determiner, adjective or adverb. Part-of-speech tagging is a fundamental task in natural language processing; see the [chapter in Juraksky & Martin's *Speech and Language Processing*](https://web.stanford.edu/~jurafsky/slp3/10.pdf) for more background about it. POS tagging is a common pre-processing step in many NLP pipeliens. For example, words with certain POS tags are more important than other words for capturing the content of a text (e.g. nouns and verbs carry more semantic meaning than grammatical words like prepositions and determiners), so models often take this into account when predicting the topic, sentiment, or some other categorical dimensions of a text. Start-of-the art models are quite successful, reaching near-perfect accuracy in the tags assigned to words. This tutorial will show you how to put together a simple tagger that uses a Recurrent Neural Network.\n",
    "\n",
    "### <font color='#6629b2'>Recurrent Neural Networks (RNNs)</font>\n",
    "\n",
    "RNNs are a general framework for modeling sequence data and are particularly useful for natural language processing tasks. At a high level, RNN encode sequences via a set of parameters (weights) that are optimized to predict some output variable. The focus of this tutorial is on the code needed to assemble a model in Keras. For a more general introduction to RNNs, see the resources at the bottom. Here an RNN will be used to encode a sentence and assign of POS tag to each word. The model shown here is applicable to any dataset with a one-to-one mapping between the inputs and outputs. This involves any task where for each sequential unit (here, a word), there is some output unit (here, a POS tag) that should be assigned to that input unit.\n",
    "\n",
    "### <font color='#6629b2'>Keras</font>\n",
    "\n",
    "[Keras](https://keras.io/) is a Python deep learning framework that lets you quickly put together neural network models with a minimal amount of code. It can be run on top of [Theano](http://deeplearning.net/software/theano/) or [Tensor Flow](https://www.tensorflow.org/) without you needing to know either of these underlying frameworks. It provides implementations of several of the layer architectures, objective functions, and optimization algorithms you need for building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Dataset</font>\n",
    "\n",
    "The [Brown Corpus](http://www.hit.uib.no/icame/brown/bcm.html) (download through NLTK [here](http://www.nltk.org/nltk_data/)) is a popular NLP resource that consists of 500 texts from a variety of sources, including news reports, academic essays, and fiction. Every word in the texts has been annotated with a POS tag. In this tutorial, each entry in the dataset is a single sentence. I split these sentences into training and testing sets of X sentences and Y sentences, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function #Python 2/3 compatibility for print statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll load the datasets using the [pandas library](https://pandas.pydata.org/), which is extremely useful for any task involving data storage and manipulation. This library puts a dataset into a readable table format, and makes it easy to retrieve specific columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Load the dataset'''\n",
    "\n",
    "import pandas\n",
    "\n",
    "train_sents = pandas.read_csv('dataset/train_brown_corpus.csv', encoding='utf-8')[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Sentence</th>\n",
       "      <th>Tagged_Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[he, was, about, 50, years, old, .]</td>\n",
       "      <td>[PRON, VERB, ADV, NUM, NOUN, ADJ, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[``, another, young, man, ,, my, dear, ?, ?]</td>\n",
       "      <td>[., DET, ADJ, NOUN, ., DET, NOUN, ., .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[really, ,, you, are, most, indiscreet, to, dr...</td>\n",
       "      <td>[ADV, ., PRON, VERB, ADV, ADJ, PRT, VERB, PRON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[delphine, presented, her, cheek, for, a, kiss...</td>\n",
       "      <td>[NOUN, VERB, DET, NOUN, ADP, DET, NOUN, ., CON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[``, dandy, is, to, be, our, house, guest, ,, ...</td>\n",
       "      <td>[., NOUN, VERB, PRT, VERB, DET, NOUN, NOUN, .,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[i, want, the, room, in, the, attic, prepared,...</td>\n",
       "      <td>[PRON, VERB, DET, NOUN, ADP, DET, NOUN, VERB, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[he, deserves, a, better, life, than, just, ro...</td>\n",
       "      <td>[PRON, VERB, DET, ADJ, NOUN, ADP, ADV, VERB, A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[``, quite, so, ,, my, dear, .]</td>\n",
       "      <td>[., ADV, ADV, ., DET, NOUN, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[his, room, will, be, ready, shortly, '', .]</td>\n",
       "      <td>[DET, NOUN, VERB, VERB, ADJ, ADV, ., .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[the, physician, led, the, horses, to, the, st...</td>\n",
       "      <td>[DET, NOUN, VERB, DET, NOUN, ADP, DET, NOUN, A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Tokenized_Sentence  \\\n",
       "0                [he, was, about, 50, years, old, .]   \n",
       "1       [``, another, young, man, ,, my, dear, ?, ?]   \n",
       "2  [really, ,, you, are, most, indiscreet, to, dr...   \n",
       "3  [delphine, presented, her, cheek, for, a, kiss...   \n",
       "4  [``, dandy, is, to, be, our, house, guest, ,, ...   \n",
       "5  [i, want, the, room, in, the, attic, prepared,...   \n",
       "6  [he, deserves, a, better, life, than, just, ro...   \n",
       "7                    [``, quite, so, ,, my, dear, .]   \n",
       "8       [his, room, will, be, ready, shortly, '', .]   \n",
       "9  [the, physician, led, the, horses, to, the, st...   \n",
       "\n",
       "                                     Tagged_Sentence  \n",
       "0               [PRON, VERB, ADV, NUM, NOUN, ADJ, .]  \n",
       "1            [., DET, ADJ, NOUN, ., DET, NOUN, ., .]  \n",
       "2  [ADV, ., PRON, VERB, ADV, ADJ, PRT, VERB, PRON...  \n",
       "3  [NOUN, VERB, DET, NOUN, ADP, DET, NOUN, ., CON...  \n",
       "4  [., NOUN, VERB, PRT, VERB, DET, NOUN, NOUN, .,...  \n",
       "5  [PRON, VERB, DET, NOUN, ADP, DET, NOUN, VERB, ...  \n",
       "6  [PRON, VERB, DET, ADJ, NOUN, ADP, ADV, VERB, A...  \n",
       "7                     [., ADV, ADV, ., DET, NOUN, .]  \n",
       "8            [DET, NOUN, VERB, VERB, ADJ, ADV, ., .]  \n",
       "9  [DET, NOUN, VERB, DET, NOUN, ADP, DET, NOUN, A...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the tokens and tags into a readable list format\n",
    "\n",
    "train_sents['Tokenized_Sentence'] = train_sents['Tokenized_Sentence'].apply(lambda sent: sent.lower().split(\"\\t\"))\n",
    "train_sents['Tagged_Sentence'] = train_sents['Tagged_Sentence'].apply(lambda sent: sent.split(\"\\t\"))\n",
    "\n",
    "train_sents[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Preparing the data</font>\n",
    "\n",
    "The sentences have already been tokenized, so both the words (tokens) in the sentence and the corresponding tags are represented as lists.\n",
    "\n",
    "We need to assemble lexicons for both the words and tags. The purpose of the lexicon is to map each word/tag to a numerical index that can be read by the model. For the words lexicon, since large datasets may contain a huge number of unique words, it's common to filter all words occurring less than a certain number of times and replace them with some generic &lt;UNK&gt; token. The min_freq parameter in the function below defines this threshold. For the tags, we'll include all of them in the model since these are the output classes we are trying to predict. There are only 11 tags in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Lexicon</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 2), ('fulton', 3), ('county', 4), ('grand', 5), ('jury', 6), ('said', 7), ('friday', 8), ('an', 9), ('investigation', 10), ('of', 11), (\"atlanta's\", 12), ('recent', 13), ('primary', 14), ('election', 15), ('produced', 16), ('``', 17), ('no', 18), ('evidence', 19), (\"''\", 20), ('that', 21)]\n",
      "[('DET', 2), ('NOUN', 3), ('ADJ', 4), ('VERB', 5), ('ADP', 6), ('.', 7), ('ADV', 8), ('CONJ', 9), ('PRT', 10), ('PRON', 11), ('NUM', 12), ('<UNK>', 1)]\n"
     ]
    }
   ],
   "source": [
    "'''Create a lexicon for the words in the sentences as well as the tags'''\n",
    "\n",
    "import pickle\n",
    "\n",
    "def make_lexicon(token_seqs, min_freq=1):\n",
    "    # First, count how often each word appears in the text.\n",
    "    token_counts = {}\n",
    "    for seq in token_seqs:\n",
    "        for token in seq:\n",
    "            if token in token_counts:\n",
    "                token_counts[token] += 1\n",
    "            else:\n",
    "                token_counts[token] = 1\n",
    "\n",
    "    # Then, assign each word to a numerical index. Filter words that occur less than min_freq times.\n",
    "    lexicon = [token for token, count in token_counts.items() if count >= min_freq]\n",
    "    # Indices start at 1. 0 is reserved for padding, and 1 is reserved for unknown words.\n",
    "    lexicon = {token:idx + 2 for idx,token in enumerate(lexicon)}\n",
    "    lexicon[u'<UNK>'] = 1 # Unknown words are those that occur fewer than min_freq times\n",
    "    lexicon_size = len(lexicon)\n",
    "\n",
    "    print(list(lexicon.items())[:20])\n",
    "    \n",
    "    return lexicon\n",
    "\n",
    "words_lexicon = make_lexicon(train_sents['Tokenized_Sentence'])\n",
    "with open('words_lexicon.pkl', 'wb') as f: #save the tokenizers\n",
    "    pickle.dump(words_lexicon, f)\n",
    "    \n",
    "tags_lexicon = make_lexicon(train_sents['Tagged_Sentence'])\n",
    "with open('tags_lexicon.pkl', 'wb') as f: #save the tokenizers\n",
    "    pickle.dump(tags_lexicon, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the model will output tags as indices, we'll obviously need to map each tag number back to its corresponding string representation in order to later interpret the output. We'll reverse the tags lexicon to create a lookup table to get each tag from its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 'DET'), (3, 'NOUN'), (4, 'ADJ'), (5, 'VERB'), (6, 'ADP'), (7, '.'), (8, 'ADV'), (9, 'CONJ'), (10, 'PRT'), (11, 'PRON'), (12, 'NUM'), (1, '<UNK>')]\n"
     ]
    }
   ],
   "source": [
    "'''Make a dictionary where the string representation of a lexicon item can be retrieved from its numerical index'''\n",
    "\n",
    "def get_lexicon_lookup(lexicon):\n",
    "    lexicon_lookup = {idx: lexicon_item for lexicon_item, idx in lexicon.items()}\n",
    "    print(list(lexicon_lookup.items())[:20])\n",
    "    return lexicon_lookup\n",
    "\n",
    "tags_lexicon_lookup = get_lexicon_lookup(tags_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>From strings to numbers</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the lexicons to transform the word and tag sequences into lists of numerical indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized_Sentence</th>\n",
       "      <th>Sentence_Idxs</th>\n",
       "      <th>Tagged_Sentence</th>\n",
       "      <th>Tag_Idxs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[the, fulton, county, grand, jury, said, frida...</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...</td>\n",
       "      <td>[DET, NOUN, NOUN, ADJ, NOUN, VERB, NOUN, DET, ...</td>\n",
       "      <td>[2, 3, 3, 4, 3, 5, 3, 2, 3, 6, 3, 4, 3, 3, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[the, jury, further, said, in, term-end, prese...</td>\n",
       "      <td>[2, 6, 27, 7, 28, 29, 30, 21, 2, 31, 32, 33, 3...</td>\n",
       "      <td>[DET, NOUN, ADV, VERB, ADP, NOUN, NOUN, ADP, D...</td>\n",
       "      <td>[2, 3, 8, 5, 6, 3, 3, 6, 2, 3, 4, 3, 7, 2, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[the, september-october, term, jury, had, been...</td>\n",
       "      <td>[2, 48, 49, 6, 36, 50, 51, 52, 3, 53, 54, 55, ...</td>\n",
       "      <td>[DET, NOUN, NOUN, NOUN, VERB, VERB, VERB, ADP,...</td>\n",
       "      <td>[2, 3, 3, 3, 5, 5, 5, 6, 3, 4, 3, 3, 3, 3, 10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[``, only, a, relative, handful, of, such, rep...</td>\n",
       "      <td>[17, 68, 69, 70, 71, 11, 72, 60, 46, 73, 20, 3...</td>\n",
       "      <td>[., ADV, DET, ADJ, NOUN, ADP, ADJ, NOUN, VERB,...</td>\n",
       "      <td>[7, 8, 2, 4, 3, 6, 4, 3, 5, 5, 7, 7, 2, 3, 5, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[the, jury, said, it, did, find, that, many, o...</td>\n",
       "      <td>[2, 6, 7, 81, 82, 83, 21, 84, 11, 85, 86, 41, ...</td>\n",
       "      <td>[DET, NOUN, VERB, PRON, VERB, VERB, ADP, ADJ, ...</td>\n",
       "      <td>[2, 3, 5, 11, 5, 5, 6, 4, 6, 3, 3, 9, 3, 3, 7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[it, recommended, that, fulton, legislators, a...</td>\n",
       "      <td>[81, 94, 21, 3, 95, 96, 17, 58, 97, 98, 87, 99...</td>\n",
       "      <td>[PRON, VERB, ADP, NOUN, NOUN, VERB, ., PRT, VE...</td>\n",
       "      <td>[11, 5, 6, 3, 3, 5, 7, 10, 5, 2, 3, 5, 9, 5, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[the, grand, jury, commented, on, a, number, o...</td>\n",
       "      <td>[2, 5, 6, 105, 106, 69, 77, 11, 107, 108, 34, ...</td>\n",
       "      <td>[DET, ADJ, NOUN, VERB, ADP, DET, NOUN, ADP, AD...</td>\n",
       "      <td>[2, 4, 3, 5, 6, 2, 3, 6, 4, 3, 7, 6, 11, 2, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[merger, proposed]</td>\n",
       "      <td>[122, 123]</td>\n",
       "      <td>[NOUN, VERB]</td>\n",
       "      <td>[3, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[however, ,, the, jury, said, it, believes, ``...</td>\n",
       "      <td>[124, 34, 2, 6, 7, 81, 125, 17, 98, 126, 127, ...</td>\n",
       "      <td>[ADV, ., DET, NOUN, VERB, PRON, VERB, ., DET, ...</td>\n",
       "      <td>[8, 7, 2, 3, 5, 11, 5, 7, 2, 12, 3, 5, 5, 5, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[the, city, purchasing, department, ,, the, ju...</td>\n",
       "      <td>[2, 31, 110, 137, 34, 2, 6, 7, 34, 17, 138, 13...</td>\n",
       "      <td>[DET, NOUN, VERB, NOUN, ., DET, NOUN, VERB, .,...</td>\n",
       "      <td>[2, 3, 5, 3, 7, 2, 3, 5, 7, 7, 5, 5, 6, 5, 4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Tokenized_Sentence  \\\n",
       "0  [the, fulton, county, grand, jury, said, frida...   \n",
       "1  [the, jury, further, said, in, term-end, prese...   \n",
       "2  [the, september-october, term, jury, had, been...   \n",
       "3  [``, only, a, relative, handful, of, such, rep...   \n",
       "4  [the, jury, said, it, did, find, that, many, o...   \n",
       "5  [it, recommended, that, fulton, legislators, a...   \n",
       "6  [the, grand, jury, commented, on, a, number, o...   \n",
       "7                                 [merger, proposed]   \n",
       "8  [however, ,, the, jury, said, it, believes, ``...   \n",
       "9  [the, city, purchasing, department, ,, the, ju...   \n",
       "\n",
       "                                       Sentence_Idxs  \\\n",
       "0  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...   \n",
       "1  [2, 6, 27, 7, 28, 29, 30, 21, 2, 31, 32, 33, 3...   \n",
       "2  [2, 48, 49, 6, 36, 50, 51, 52, 3, 53, 54, 55, ...   \n",
       "3  [17, 68, 69, 70, 71, 11, 72, 60, 46, 73, 20, 3...   \n",
       "4  [2, 6, 7, 81, 82, 83, 21, 84, 11, 85, 86, 41, ...   \n",
       "5  [81, 94, 21, 3, 95, 96, 17, 58, 97, 98, 87, 99...   \n",
       "6  [2, 5, 6, 105, 106, 69, 77, 11, 107, 108, 34, ...   \n",
       "7                                         [122, 123]   \n",
       "8  [124, 34, 2, 6, 7, 81, 125, 17, 98, 126, 127, ...   \n",
       "9  [2, 31, 110, 137, 34, 2, 6, 7, 34, 17, 138, 13...   \n",
       "\n",
       "                                     Tagged_Sentence  \\\n",
       "0  [DET, NOUN, NOUN, ADJ, NOUN, VERB, NOUN, DET, ...   \n",
       "1  [DET, NOUN, ADV, VERB, ADP, NOUN, NOUN, ADP, D...   \n",
       "2  [DET, NOUN, NOUN, NOUN, VERB, VERB, VERB, ADP,...   \n",
       "3  [., ADV, DET, ADJ, NOUN, ADP, ADJ, NOUN, VERB,...   \n",
       "4  [DET, NOUN, VERB, PRON, VERB, VERB, ADP, ADJ, ...   \n",
       "5  [PRON, VERB, ADP, NOUN, NOUN, VERB, ., PRT, VE...   \n",
       "6  [DET, ADJ, NOUN, VERB, ADP, DET, NOUN, ADP, AD...   \n",
       "7                                       [NOUN, VERB]   \n",
       "8  [ADV, ., DET, NOUN, VERB, PRON, VERB, ., DET, ...   \n",
       "9  [DET, NOUN, VERB, NOUN, ., DET, NOUN, VERB, .,...   \n",
       "\n",
       "                                            Tag_Idxs  \n",
       "0  [2, 3, 3, 4, 3, 5, 3, 2, 3, 6, 3, 4, 3, 3, 5, ...  \n",
       "1  [2, 3, 8, 5, 6, 3, 3, 6, 2, 3, 4, 3, 7, 2, 5, ...  \n",
       "2  [2, 3, 3, 3, 5, 5, 5, 6, 3, 4, 3, 3, 3, 3, 10,...  \n",
       "3  [7, 8, 2, 4, 3, 6, 4, 3, 5, 5, 7, 7, 2, 3, 5, ...  \n",
       "4  [2, 3, 5, 11, 5, 5, 6, 4, 6, 3, 3, 9, 3, 3, 7,...  \n",
       "5  [11, 5, 6, 3, 3, 5, 7, 10, 5, 2, 3, 5, 9, 5, 6...  \n",
       "6  [2, 4, 3, 5, 6, 2, 3, 6, 4, 3, 7, 6, 11, 2, 3,...  \n",
       "7                                             [3, 5]  \n",
       "8  [8, 7, 2, 3, 5, 11, 5, 7, 2, 12, 3, 5, 5, 5, 1...  \n",
       "9  [2, 3, 5, 3, 7, 2, 3, 5, 7, 7, 5, 5, 6, 5, 4, ...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_to_idxs(token_seqs, lexicon):\n",
    "    idx_seqs = [[lexicon[token] if token in lexicon else lexicon['<UNK>'] for token in token_seq]  \n",
    "                                                                     for token_seq in token_seqs]\n",
    "    return idx_seqs\n",
    "\n",
    "train_sents['Sentence_Idxs'] = tokens_to_idxs(train_sents['Tokenized_Sentence'], words_lexicon)\n",
    "\n",
    "train_sents['Tag_Idxs'] = tokens_to_idxs(train_sents['Tagged_Sentence'], tags_lexicon)\n",
    "\n",
    "train_sents[['Tokenized_Sentence', 'Sentence_Idxs', 'Tagged_Sentence', 'Tag_Idxs']][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color='#6629b2'>Numerical lists to matrices</font>\n",
    "\n",
    "Finally, we need to put the input sequences into matrices for training. There will be separate matrices for the word and tag sequences, where each row is a sentence and each column is a word (or tag) index in that sentence. This matrix format is necessary for the model to process the sentences in batches as opposed to one at a time, which significantly speeds up training. However, each sentence has a different number of words, so we create a padded matrix equal to the length on the longest sentence in the training set. For all sentences with fewer words, we prepend the row with zeros representing an empty word (and tag) position. We can specify to Keras to ignore these zeros during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDS: [[  0   0   0 ...,  24  25  26]\n",
      " [  0   0   0 ...,  46  47  26]\n",
      " [  0   0   0 ...,  66  67  26]\n",
      " ..., \n",
      " [  0   0   0 ..., 758  20  26]\n",
      " [  0   0   0 ..., 802  34 447]\n",
      " [  0   0   0 ..., 447 812  26]]\n",
      "SHAPE: (100, 60) \n",
      "\n",
      "TAGS: [[0 0 0 ..., 5 3 7]\n",
      " [0 0 0 ..., 5 5 7]\n",
      " [0 0 0 ..., 3 3 7]\n",
      " ..., \n",
      " [0 0 0 ..., 3 7 7]\n",
      " [0 0 0 ..., 3 7 3]\n",
      " [0 0 0 ..., 3 3 7]]\n",
      "SHAPE: (100, 60) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_idx_seqs(idx_seqs, max_seq_len):\n",
    "    # Keras provides a convenient padding function; \n",
    "    padded_idxs = pad_sequences(sequences=idx_seqs, maxlen=max_seq_len)\n",
    "    return padded_idxs\n",
    "\n",
    "max_seq_len = max([len(idx_seq) for idx_seq in train_sents['Sentence_Idxs']]) # Get length of longest sequence\n",
    "train_padded_words = pad_idx_seqs(train_sents['Sentence_Idxs'], \n",
    "                                  max_seq_len + 1) #Add one to max length for offsetting sequence by 1\n",
    "train_padded_tags = pad_idx_seqs(train_sents['Tag_Idxs'],\n",
    "                                 max_seq_len + 1)  #Add one to max length for offsetting sequence by 1\n",
    "\n",
    "print(\"WORDS:\", train_padded_words)\n",
    "print(\"SHAPE:\", train_padded_words.shape, \"\\n\")\n",
    "\n",
    "print(\"TAGS:\", train_padded_tags)\n",
    "print(\"SHAPE:\", train_padded_tags.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Defining the input and output</font>\n",
    "\n",
    "In this approach, for each word in a sentence, we predict the tag for that word based on two types of input: 1. all the words in the sentence up to that point, including that current word, and 2. all the previous tags in the sentence. So for a given position in the sentence *idx*, the input is train_padded_words[idx] and train_padded_tags[idx-1], and the output is train_padded_tags[idx]. In other words, the input tags matrix will be offset by one to the left. The example below shows this alignment for the first sentence in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-51b478b989be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m pandas.DataFrame(list(zip(train_sents['Tokenized_Sentence'].loc[0],\n\u001b[0;32m----> 4\u001b[0;31m                           \u001b[0;34m[\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tagged_Sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                           train_sents['Tagged_Sentence'].loc[0])),\n\u001b[1;32m      6\u001b[0m                  columns=['Input Word', 'Input Tag', 'Output Tag'])\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "pandas.DataFrame(list(zip(train_sents['Tokenized_Sentence'].loc[0],\n",
    "                          [\"-\"] + train_sents['Tagged_Sentence'].loc[0],\n",
    "                          train_sents['Tagged_Sentence'].loc[0])),\n",
    "                 columns=['Input Word', 'Input Tag', 'Output Tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, the input tag sequences are offset by one so that each tag is predicted based on the tag sequence up to that point. To keep the matrices the same length, the input word matrix and output tag matrix will both be offset by one to the right. Thus the length of all matrices will be reduced by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Input Words  Input Tags  Output Tags\n",
      "0             0           0            0\n",
      "1             0           0            0\n",
      "2             0           0            0\n",
      "3             0           0            0\n",
      "4             0           0            0\n",
      "5             0           0            0\n",
      "6             0           0            0\n",
      "7             0           0            0\n",
      "8             0           0            0\n",
      "9             0           0            0\n",
      "10            0           0            0\n",
      "11            0           0            0\n",
      "12            0           0            0\n",
      "13            0           0            0\n",
      "14            0           0            0\n",
      "15            0           0            0\n",
      "16            0           0            0\n",
      "17            0           0            0\n",
      "18            0           0            0\n",
      "19            0           0            0\n",
      "20            0           0            0\n",
      "21            0           0            0\n",
      "22            0           0            0\n",
      "23            0           0            0\n",
      "24            0           0            0\n",
      "25            0           0            0\n",
      "26            0           0            0\n",
      "27            0           0            0\n",
      "28            0           0            0\n",
      "29            0           0            0\n",
      "30            0           0            0\n",
      "31            0           0            0\n",
      "32            0           0            0\n",
      "33            0           0            0\n",
      "34            2           0            2\n",
      "35            3           2            3\n",
      "36            4           3            3\n",
      "37            5           3            4\n",
      "38            6           4            3\n",
      "39            7           3            5\n",
      "40            8           5            3\n",
      "41            9           3            2\n",
      "42           10           2            3\n",
      "43           11           3            6\n",
      "44           12           6            3\n",
      "45           13           3            4\n",
      "46           14           4            3\n",
      "47           15           3            3\n",
      "48           16           3            5\n",
      "49           17           5            7\n",
      "50           18           7            2\n",
      "51           19           2            3\n",
      "52           20           3            7\n",
      "53           21           7            6\n",
      "54           22           6            2\n",
      "55           23           2            3\n",
      "56           24           3            5\n",
      "57           25           5            3\n",
      "58           26           3            7\n"
     ]
    }
   ],
   "source": [
    "print(pandas.DataFrame(list(zip(train_padded_words[0,1:], train_padded_tags[0,:-1], train_padded_tags[0, 1:])),\n",
    "                columns=['Input Words', 'Input Tags', 'Output Tags']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color='#6629b2'>Building the model</font>\n",
    "\n",
    "### <font color='#6629b2'>Functional API</font>\n",
    "\n",
    "To set up the model, we'll use Keras [Functional API](https://keras.io/getting-started/functional-api-guide/), which is one of two ways to assemble models in Keras (the alternative is the [Sequential API](https://keras.io/getting-started/sequential-model-guide/), which is a bit simpler but has more constraints). For the POS tagger model, new tags will be predicted from the combination of two input sequences, the words in the sentence and the corresponding tags in the sentence. The Functional API is specifically useful when a model has multiple inputs and/or outputs. Compared to the Sequential model where the order in which the layers are added indicates which layers are connected, in the Functional API, the input to a particular layer must be specified as a parameter, e.g. Embedding()(word_input). This is what enables the model to have multiple inputs and outputs. A model is defined with the Model() function, and there the list of inputs and outputs is explicitly given. \n",
    "\n",
    "### <font color='#6629b2'>Layers</font>\n",
    "\n",
    "We'll build an RNN with the following layers, numbered according to the level on which they are stacked:\n",
    "\n",
    "**1. Input (words)**: This input layer takes in a sequence of word indices.\n",
    "\n",
    "**1. Input (tags)**: This is the other input layer alongside the first, and it takes in a sequence of tag indices. It is on the same level as the word input layer, so both input sequences are read in parallel by the model.\n",
    "\n",
    "**2. Embedding (words)**: There are two embedding layers, one for the words and a different one for the tags. Both of them function the same way: they convert the indices into distributed vector representations (embeddings). The mask_zero=True parameter indicates that values of 0 in the matrix (the padding) will be ignored by the model.\n",
    "\n",
    "**2. Embedding (tags)**: Same as the word embedding layer, but for the tags.\n",
    "\n",
    "**3. Concatenate**: This layer merges each embedded word sequence and corresponding embedded tag sequence into a single sequence. This means that for a given word and the tag for that word, their vectors will be concatenated into a single vector.\n",
    "\n",
    "**4. GRU**: The recurrent (GRU) hidden layer reads the merged embedded sequence and computes a representation (hidden state) of the sequence. The result is a new vector for each word/tag in the sequence. There are a few architectures for this layer - I use the GRU variation, Keras also provides LSTM or just the simple vanilla recurrent layer. By specifying return_sequences=True in the below function, this layer will output the entire sequence of vectors (hidden states) for the sequence, rather than just the most recent hidden state that is returned by default.\n",
    "\n",
    "**5. (Time Distributed) Dense**: An output layer that produces a probability distribution for each possible tag for each word in the sequence. The 'softmax' activation is what transforms the values of this layer into scores from 0 to 1 that can be treated as probabilities. The Dense layer produces the probability scores for one particular timepoint (word). By wrapping this in a TimeDistributed() layer, the model outputs a probability distribution for every timepoint in the sequence. \n",
    "\n",
    "Each layer is connected to the layer above it via a set of weights, which are the parameters that are adjusted during training in order for the model to learn to predict tags. \n",
    "\n",
    "### <font color='#6629b2'>Parameters</font>\n",
    "\n",
    "Our function for creating the model takes the following parameters:\n",
    "\n",
    "**seq_input_length**: the length of the padded matrices for the word and tag sentence inputs, which will be the same since there is a one-to-one mapping between tags. This is equal to the length of the longest sentence in the training data. \n",
    "\n",
    "**n_word_input_nodes**: the number of unique words in the lexicon, plus one to account for matrix padding represented by 0 values. This indicates the number of rows in the word embedding layer, where each row corresponds to a word.\n",
    "\n",
    "**n_tag_input_nodes**: the number of unique tags in the dataset, plus one to account for padding. This indicates the number of rows in the tag embedding layer, where each row corresponds to a tag.\n",
    "\n",
    "**n_word_embedding_nodes**: the number of dimensions in the word embedding layer, which can be freely defined. Here, it is set to 300.\n",
    "\n",
    "**n_tag_embedding_nodes**: the number of dimensions in the tag embedding layer, which can be freely defined. Here, it is set to 100.\n",
    "\n",
    "**n_hidden_nodes**: the number of dimensions in the hidden layer. Like the embedding layers, this can be freely chosen. Here, it is set to 500.\n",
    "\n",
    "**stateful**: By default, the GRU hidden layer will reset its state (i.e. its values will be 0s) each time a new set of sequences is read into the model.  However, when stateful=True is given, this parameter indicates that the GRU hidden layer should \"remember\" its state until it is explicitly told to forget it. In other words, the values in this layer will be carried over between separate calls to the training function. This is useful when processing long sequences, so that the model can iterate through chunks of the sequences rather than loading the entire matrix at the same time, which is memory-intensive. I'll show below how this setting is also useful when tagging new sequences. Here, because the training sequences only consist of one sentence, stateful will be set to False during training. At prediction time, it will be set to True.\n",
    "\n",
    "**batch_size**: It is not always necessary to specify the batch size when setting up a Keras model. The fit() function will apply batch processing by default and the batch size can be given as a parameter. However, when a model is stateful, the batch size does need to be specified in the Input() layers. Here, for training, batch_size=None, so Keras will use its default batch size (which is 32). During prediction, the batch size will be set to 1.\n",
    "\n",
    "### <font color='#6629b2'>Procedure</font>\n",
    "\n",
    "The output of the model is a sequence of vectors, each with the same number of dimensions as the number of unique tags (n_tag_input_nodes). Each vector contains the predicted probability of each possible tag for the corresponding word in that position in the sequence. Like all neural networks, RNNs learn by updating the parameters (weights) to optimize an objective (loss) function applied to the output. For this model, the objective is to minimize the cross entropy (named as the \"sparse_categorical_crossentropy\" in the code) between the predicted tag probabilities and the probabilities observed from the words in training data, resulting in probabilities that more accurately predict when a particular tag will appear. This is the general procedure used for all multi-label classification tasks. Updates to the weights of the model are performed using an optimization algorithm, such as Adam used here. The details of this process are extensive; see the resources at the bottom of the notebook if you want a deeper understanding. One huge benefit of Keras is that it implements many of these details for you. Not only does it already have implementations of the types of layer architectures, it also has many of the [loss functions](https://keras.io/losses/) and [optimization methods](https://keras.io/optimizers/) you need for training various models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "'''Create the model'''\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Concatenate, TimeDistributed, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "\n",
    "def create_model(seq_input_len, n_word_input_nodes, n_tag_input_nodes, n_word_embedding_nodes,\n",
    "                 n_tag_embedding_nodes, n_hidden_nodes, stateful=False, batch_size=None):\n",
    "    \n",
    "    #Layers 1\n",
    "    word_input = Input(batch_shape=(batch_size, seq_input_len))\n",
    "    tag_input = Input(batch_shape=(batch_size, seq_input_len))\n",
    "\n",
    "    #Layer 2\n",
    "    word_embeddings = Embedding(input_dim=n_word_input_nodes,\n",
    "                                output_dim=n_word_embedding_nodes, \n",
    "                                mask_zero=True)(word_input)\n",
    "    #Output shape = (batch_size, seq_input_len, n_word_embedding_nodes)\n",
    "    tag_embeddings = Embedding(input_dim=n_tag_input_nodes,\n",
    "                               output_dim=n_tag_embedding_nodes,\n",
    "                               mask_zero=True)(tag_input) \n",
    "    #Output shape = (batch_size, seq_input_len, n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 3\n",
    "    merged_embeddings = Concatenate(axis=-1)([word_embeddings, tag_embeddings])\n",
    "    #Output shape =  (batch_size, seq_input_len, n_word_embedding_nodes + n_tag_embedding_nodes)\n",
    "    \n",
    "    #Layer 4\n",
    "    hidden_layer = GRU(units=n_hidden_nodes, return_sequences=True, stateful=stateful)(merged_embeddings)\n",
    "    #Output shape = (batch_size, seq_input_len, n_hidden_nodes)\n",
    "    \n",
    "    #Layer 5\n",
    "    output_layer = TimeDistributed(Dense(units=n_tag_input_nodes, activation='softmax'))(hidden_layer)\n",
    "    # Output shape = (batch_size, seq_input_len, n_tag_input_nodes)\n",
    "    \n",
    "    #Specify which layers are input and output, compile model with loss and optimization functions\n",
    "    model = Model(inputs=[word_input, tag_input], outputs=output_layer)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                  optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(seq_input_len=train_padded_words.shape[-1] - 1, #substract 1 from matrix length because of offset\n",
    "                     n_word_input_nodes=len(tokens_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_tag_input_nodes=len(tags_lexicon) + 1, #Add one for 0 padding\n",
    "                     n_word_embedding_nodes=300,\n",
    "                     n_tag_embedding_nodes=100,\n",
    "                     n_hidden_nodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Training</font>\n",
    "\n",
    "Now we're ready to train the model. We'll call the fit() function to train the model for 10 iterations through the dataset (epochs). Keras reports to cross-entropy loss after each epoch, which should continue to decrease if the model is learning correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Train the model'''\n",
    "\n",
    "# output matrix (y) has extra 3rd dimension added because sparse cross-entropy function requires one label per row\n",
    "model.fit(x=[train_padded_words[:,1:], train_padded_tags[:,:-1]], y=train_padded_tags[:, 1:, None], epochs=10)\n",
    "model.save_weights('model_weights.h5') #Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Tagging new sentences</font>\n",
    "\n",
    "Now that the model is trained, it can be used to predict tags in new sentences in the test set. As opposed to training where we processed multiple sentences at the same time, it will be more straightforward to demonstrate tagging on a single sentence at a time. In Keras, you can duplicate a model by loading the parameters from a saved model into a new model. Here, this new model will have a batch size of 1. It will also process a sentence one word/tag at a time (seq_input_len=1) and predict the next tag, using the stateful=True parameter to remember its previous predictions within that sentence. The other parameters of this prediction model are exactly the same as the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Load the test set and apply same processing steps performed above for training set'''\n",
    "\n",
    "test_sents = pandas.read_csv('dataset/test_brown_corpus.csv', encoding='utf-8')[:100] #Load sample of 100 sentences\n",
    "test_sents['Tokenized_Sentence'] = test_sents['Tokenized_Sentence'].apply(lambda sent: sent.lower().split(\"\\t\"))\n",
    "test_sents['Tagged_Sentence'] = test_sents['Tagged_Sentence'].apply(lambda sent: sent.split(\"\\t\"))\n",
    "test_sents['Sentence_Idxs'] = tokens_to_idxs(test_sents['Tokenized_Sentence'], words_lexicon)\n",
    "test_sents['Tag_Idxs'] = tokens_to_idxs(test_sents['Tagged_Sentence'], tags_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_lexicon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-8964679c7cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m predictor_model = create_model(seq_input_len=1,\n\u001b[0;32m----> 4\u001b[0;31m                                \u001b[0mn_token_input_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_lexicon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m                                \u001b[0mn_tag_input_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags_lexicon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                \u001b[0mn_token_embedding_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens_lexicon' is not defined"
     ]
    }
   ],
   "source": [
    "'''Duplicate the trained model, setting batch_size = 1, seq_input_len = 1 and stateful=True'''\n",
    "\n",
    "predictor_model = create_model(seq_input_len=1,\n",
    "                               n_token_input_nodes=len(tokens_lexicon) + 1,\n",
    "                               n_tag_input_nodes=len(tags_lexicon) + 1,\n",
    "                               n_token_embedding_nodes=300,\n",
    "                               n_tag_embedding_nodes=100,\n",
    "                               n_hidden_nodes=500,\n",
    "                               stateful=True,\n",
    "                               batch_size=1)\n",
    "\n",
    "#Transfer the weights from the trained model\n",
    "predictor_model.set_weights(model.get_weights()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll iterate through the sentences in the test set and tag each of them. For each sentence, we start with an empty list for the predicted tags. For the first word in the sentence, there is no previous tag, so the model reads that word and the empty tag 0 (the padding value). The predict() function returns a probability distribution over the tags, and we pick the tag with the highest probability as the one to assign that word. This tag is appended to our list of predicted tags, and we continue to the next word in the sentence. Because the model is stateful, we can simply provide the current word and most recent tag as input to the predict() function, since it has saved the state of the model after the previous prediction. After the entire sentence has been tagged, we call reset_states() to clear the values for this sentence so we can process a new sentence. The tag indices are mapped back to their string forms, which we show in the sample below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import pdb;pdb.set_trace()\n",
    "    pred_tags = []\n",
    "    for _, sent in test_sents.iterrows()[:10]:\n",
    "        tok_sent = sent['Tokenized_Sentence']\n",
    "        sent_idxs = sent['Sentence_Idxs']\n",
    "        sent_gold_tags = sent['Tagged_Sentence']\n",
    "        sent_pred_tags = []\n",
    "        prev_tag = 0  #initialize predicted tag sequence with padding\n",
    "        for cur_word in sent_idxs:\n",
    "            # cur_word and prev_tag are just integers, but the model expects an input array\n",
    "            # with the shape (batch_size, seq_input_len), so prepend two dimensions to these values\n",
    "            p_next_tag = predictor_model.predict(x=[numpy.array(cur_word)[None, None],\n",
    "                                                        numpy.array(prev_tag)[None, None]])[0]\n",
    "            prev_tag = numpy.argmax(p_next_tag, axis=-1)[0]\n",
    "            sent_pred_tags.append(prev_tag)\n",
    "        predictor_model.reset_states()\n",
    "        \n",
    "        #Map tags back to string labels\n",
    "        sent_pred_tags = [tags_lexicon_lookup[tag] for tag in sent_pred_tags]\n",
    "        pred_tags.append(sent_pred_tags) #filter padding\n",
    "        \n",
    "        print(\"SENTENCE:\\t{}\".format(\"\\t\".join(tok_sent)))\n",
    "        print(\"PREDICTED:\\t{}\".format(\"\\t\".join(sent_pred_tags)))\n",
    "        print(\"GOLD:\\t\\t{}\".format(\"\\t\".join(sent_gold_tags)), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#6629b2'>Evaluation</font>\n",
    "\n",
    "We can evaluate our model with some of the standard metrics for classification: *precision*, *recall*, and *F1 score*. In the context of this task, precision is the proportion of the predicted tags for a particular class that were correct predictions (i.e. of all the words that were assigned a NOUN tag by the tagger, what percentage of these were actually nouns according to the test set?). Recall is the proportion of correct tags for a particular class that the tagger also predicted correctly (i.e. of all the words in the test set that should have been assigned a NOUN tag, what percentage of these were actually tagged as a NOUN?). F1 score is a weighted average of precision and recall. The scikit-learn package has several of these [evaluation metrics](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics) available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-65b481fdb71d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_gold_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent_tags\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tagged_Sentence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mall_pred_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent_tags\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_tags\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_gold_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_pred_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_gold_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_pred_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_tags' is not defined"
     ]
    }
   ],
   "source": [
    "'''Evalute the model by precision, recall, and F1'''\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "all_gold_tags = [tag for sent_tags in test_sents['Tagged_Sentence'] for tag in sent_tags]\n",
    "all_pred_tags = [tag for sent_tags in pred_tags for tag in sent_tags]\n",
    "accuracy = accuracy_score(y_true=all_gold_tags, y_pred=all_pred_tags)\n",
    "precision = precision_score(y_true=all_gold_tags, y_pred=all_pred_tags, average='weighted')\n",
    "recall = recall_score(y_true=all_gold_tags, y_pred=all_pred_tags, average='weighted')\n",
    "f1 = f1_score(y_true=all_gold_tags, y_pred=all_pred_tags, average='weighted')\n",
    "\n",
    "print(\"ACCURACY: {:.3f}\".format(accuracy))\n",
    "print(\"PRECISION: {:.3f}\".format(precision))\n",
    "print(\"RECALL: {:.3f}\".format(recall))\n",
    "print(\"F1: {:.3f}\".format(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>Conclusion</font>\n",
    "\n",
    "Even though this model can accuractely predict many POS tags, state-of-the-art taggers use more sophisticated techniques. For example, where here we predicted a tag just based on the preceding words and tags, [bidirectional layers](https://keras.io/layers/wrappers/#bidirectional) also model the sequence that appears after the given word to additionally inform the prediction. POS tagging can be seen as a shallow version of syntactic parsing, which is a more difficult NLP problem. Where POS tagging outputs a flat sequence with a one-to-one mapping between words and tags, syntatic parsing produces a hierarchical structure where categories consist of multiple-word phrases and phrase categories are embedded inside other phrases. Check out the [chapter from Jurafsky & Martin's book](https://web.stanford.edu/~jurafsky/slp3/14.pdf) if you're interested in learning more about these deeper models of linguistic structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='#6629b2'>More resources</font>\n",
    "\n",
    "Yoav Goldberg's book [Neural Network Methods for Natural Language Processing](http://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037) is a thorough introduction to neural networks for NLP tasks in general\n",
    "\n",
    "If you'd like to learn more about what Keras is doing under the hood, the [Theano tutorials](http://deeplearning.net/tutorial/) are useful. There is one specifically on [semantic parsing](http://deeplearning.net/tutorial/rnnslu.html#rnnslu), which is related to the POS tagging task.\n",
    "\n",
    "TensorFlow also has an RNN language model [tutorial](https://www.tensorflow.org/versions/r0.12/tutorials/recurrent/index.html) using the Penn Treebank dataset\n",
    "\n",
    "Andrej Karpathy's blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) is very helpful for understanding the underlying details of the same language model I've demonstrated here. It also provides raw Python code with an implementation of the backpropagation algorithm.\n",
    "\n",
    "Chris Olah provides a good [explanation](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) of how LSTM RNNs work (this explanation also applies to the GRU model used here)\n",
    "\n",
    "Denny Britz's [tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) documents well both the technical details of RNNs and their implementation in Python.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
